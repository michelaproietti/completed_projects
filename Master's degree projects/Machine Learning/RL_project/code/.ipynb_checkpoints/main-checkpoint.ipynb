{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from policy import Policy\n",
    "from value import NNValueFunction\n",
    "import scipy.signal\n",
    "from utils import Logger, Scaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single episode: it returns a scalar for the entropy\n",
    "# and a 4-tuple of NumPy arrays for observations, actions, rewards and unscaled observations \n",
    "def run_episode(env, policy, scaler, animate=False): # animate: if True uses env.render() to animate episode\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, unscaled_observes = [], [], [], []\n",
    "    done = False\n",
    "    step = 0.0\n",
    "    scale, offset = scaler.get()\n",
    "    scale[-1] = 1.0  # don't scale time step feature\n",
    "    offset[-1] = 0.0  # don't offset time step feature\n",
    "    last_action = None\n",
    "    entropy = 0\n",
    "    while not done:\n",
    "        if animate:\n",
    "            env.render()\n",
    "        obs = np.concatenate([obs, [step]])  # add time step feature\n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        unscaled_observes.append(obs)\n",
    "        obs = np.float32((obs - offset) * scale)  # center and scale observations\n",
    "        observes.append(obs)\n",
    "        action, action_prob = policy.sample(obs, last_action, env) # draw a sample from the policy\n",
    "        entropy += -tf.reduce_sum(action_prob*tf.math.log(action_prob))\n",
    "        actions.append(action)\n",
    "        obs, reward, done, _ = env.step(action) # perform a step in the enviroment and get the \n",
    "                                                # relative observation and reward\n",
    "        rewards.append(reward)\n",
    "        step += 1e-3  # increment time step feature\n",
    "    return (np.concatenate(observes), # observes: shape = (episode len, obs_dim)\n",
    "            actions, # actions: shape = list of actions\n",
    "            np.array(rewards, dtype=np.float32), #rewards: shape = (episode len,)\n",
    "            np.concatenate(unscaled_observes), # unscaled_observes: useful for training scaler, shape = (episode len, obs_dim)\n",
    "            entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run policy: we run a fixed number of episodes and we collect the relative trajectories\n",
    "def run_policy(env, policy, scaler, logger, episodes):\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, unscaled_observes, entropy = run_episode(env, policy, scaler)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes, 'actions': actions, 'rewards': rewards,\n",
    "                      'unscaled_observes': unscaled_observes}\n",
    "        trajectories.append(trajectory)\n",
    "    unscaled = np.concatenate([t['unscaled_observes'] for t in trajectories])\n",
    "    scaler.update(unscaled)  # update running statistics for scaling observations\n",
    "    if logger is not None:\n",
    "        logger.log({'_MeanReward': np.mean([t['rewards'].sum() for t in trajectories]), 'Steps': total_steps})\n",
    "\n",
    "    return trajectories, entropy # Returns: the entropy and a list of trajectory dictionaries (observes, actions, rewards, unscaled_observes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds estimated value to all time steps of all trajectories\n",
    "def add_value(trajectories, value_func): # value_func: takes observations and returns predicted state value\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = value_func.predict(observes)\n",
    "        trajectory['values'] = values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute discounted forward sum of a sequence at each point\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
    "\n",
    "# Compute thte discount sum rewards and the advantages of all trajectories\n",
    "def add_discount_sum_reward_and_advantage(trajectories, gamma):\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        disc_sum_rew = discount(rewards, gamma)\n",
    "        trajectory['disc_sum_rew'] = disc_sum_rew\n",
    "        values = trajectory['values']\n",
    "        \n",
    "        # temporal difference error to estimate advantage\n",
    "        advantages = rewards - values + np.append(values[1:] * gamma, 0)\n",
    "        trajectory['advantages'] = advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log various batch statistics\n",
    "def log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode):\n",
    "    logger.log({'_mean_obs': np.mean(observes), '_min_obs': np.min(observes),\n",
    "                '_max_obs': np.max(observes), '_std_obs': np.mean(np.var(observes, axis=0)),\n",
    "                '_mean_act': np.mean(actions), '_min_act': np.min(actions),\n",
    "                '_max_act': np.max(actions), '_std_act': np.mean(np.var(actions, axis=0)),\n",
    "                '_mean_adv': np.mean(advantages), '_min_adv': np.min(advantages),\n",
    "                '_max_adv': np.max(advantages), '_std_adv': np.var(advantages),\n",
    "                '_mean_discrew': np.mean(disc_sum_rew), '_min_discrew': np.min(disc_sum_rew),\n",
    "                '_max_discrew': np.max(disc_sum_rew), '_std_discrew': np.var(disc_sum_rew), '_Episode': episode})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Params -- h1: 144, h2: 26, h3: 5, lr: 0.00196\n",
      "Policy Params -- h1: 144, h2: 58\n",
      "Linesearch worked at  0\n",
      "***** Episode 20, Mean R = -707.4 *****\n",
      "ExplainedVarNew: -0.886\n",
      "ExplainedVarOld: -0.177\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.79e+03\n",
      "PolicyLoss: 1.79\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0504\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 40, Mean R = -783.5 *****\n",
      "ExplainedVarNew: -0.886\n",
      "ExplainedVarOld: -0.708\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.78e+03\n",
      "PolicyLoss: 1.78\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0598\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 60, Mean R = -789.1 *****\n",
      "ExplainedVarNew: -1.05\n",
      "ExplainedVarOld: -1.02\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.79e+03\n",
      "PolicyLoss: 1.79\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0585\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 80, Mean R = -641.1 *****\n",
      "ExplainedVarNew: -1.08\n",
      "ExplainedVarOld: -1.07\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.78e+03\n",
      "PolicyLoss: 1.78\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0603\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 100, Mean R = -556.8 *****\n",
      "ExplainedVarNew: -1.03\n",
      "ExplainedVarOld: -1.63\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.76e+03\n",
      "PolicyLoss: 1.76\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0404\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 120, Mean R = -578.5 *****\n",
      "ExplainedVarNew: -0.986\n",
      "ExplainedVarOld: -0.982\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.74e+03\n",
      "PolicyLoss: 1.74\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0424\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 140, Mean R = -512.8 *****\n",
      "ExplainedVarNew: -1.04\n",
      "ExplainedVarOld: -1.21\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.71e+03\n",
      "PolicyLoss: 1.71\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0352\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 160, Mean R = -516.5 *****\n",
      "ExplainedVarNew: -0.954\n",
      "ExplainedVarOld: -1.1\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.68e+03\n",
      "PolicyLoss: 1.68\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0326\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 180, Mean R = -498.2 *****\n",
      "ExplainedVarNew: -0.93\n",
      "ExplainedVarOld: -0.974\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.63e+03\n",
      "PolicyLoss: 1.63\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0314\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 200, Mean R = -446.6 *****\n",
      "ExplainedVarNew: -0.897\n",
      "ExplainedVarOld: -0.986\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.57e+03\n",
      "PolicyLoss: 1.57\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0293\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 220, Mean R = -424.8 *****\n",
      "ExplainedVarNew: -1.01\n",
      "ExplainedVarOld: -0.976\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.53e+03\n",
      "PolicyLoss: 1.53\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0283\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 240, Mean R = -390.3 *****\n",
      "ExplainedVarNew: -1.03\n",
      "ExplainedVarOld: -1.26\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.5e+03\n",
      "PolicyLoss: 1.5\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0229\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 260, Mean R = -345.7 *****\n",
      "ExplainedVarNew: -0.907\n",
      "ExplainedVarOld: -1.03\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.45e+03\n",
      "PolicyLoss: 1.45\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0211\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 280, Mean R = -308.6 *****\n",
      "ExplainedVarNew: -0.856\n",
      "ExplainedVarOld: -0.843\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.35e+03\n",
      "PolicyLoss: 1.35\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0219\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 300, Mean R = -286.1 *****\n",
      "ExplainedVarNew: -0.838\n",
      "ExplainedVarOld: -0.9\n",
      "KL: -6e-08\n",
      "PolicyEntropy: 1.3e+03\n",
      "PolicyLoss: 1.3\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0207\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 320, Mean R = -281.7 *****\n",
      "ExplainedVarNew: -0.802\n",
      "ExplainedVarOld: -0.833\n",
      "KL: -5.73e-08\n",
      "PolicyEntropy: 1.24e+03\n",
      "PolicyLoss: 1.24\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0204\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 340, Mean R = -266.6 *****\n",
      "ExplainedVarNew: -0.89\n",
      "ExplainedVarOld: -0.777\n",
      "KL: -5.5e-08\n",
      "PolicyEntropy: 1.25e+03\n",
      "PolicyLoss: 1.26\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.022\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 360, Mean R = -269.3 *****\n",
      "ExplainedVarNew: -0.863\n",
      "ExplainedVarOld: -0.878\n",
      "KL: -5.42e-08\n",
      "PolicyEntropy: 1.24e+03\n",
      "PolicyLoss: 1.27\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0221\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 380, Mean R = -256.2 *****\n",
      "ExplainedVarNew: -0.868\n",
      "ExplainedVarOld: -0.801\n",
      "KL: -5.37e-08\n",
      "PolicyEntropy: 1.22e+03\n",
      "PolicyLoss: 1.25\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0241\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 400, Mean R = -233.1 *****\n",
      "ExplainedVarNew: -0.896\n",
      "ExplainedVarOld: -0.935\n",
      "KL: -5e-08\n",
      "PolicyEntropy: 1.22e+03\n",
      "PolicyLoss: 1.26\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0225\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 420, Mean R = -224.5 *****\n",
      "ExplainedVarNew: -0.898\n",
      "ExplainedVarOld: -0.875\n",
      "KL: -5e-08\n",
      "PolicyEntropy: 1.24e+03\n",
      "PolicyLoss: 1.29\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0232\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 440, Mean R = -158.2 *****\n",
      "ExplainedVarNew: -0.953\n",
      "ExplainedVarOld: -1.22\n",
      "KL: -5e-08\n",
      "PolicyEntropy: 1.22e+03\n",
      "PolicyLoss: 1.27\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0182\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 460, Mean R = -164.6 *****\n",
      "ExplainedVarNew: -0.727\n",
      "ExplainedVarOld: -0.666\n",
      "KL: -5e-08\n",
      "PolicyEntropy: 1.16e+03\n",
      "PolicyLoss: 1.21\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.022\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 480, Mean R = -92.1 *****\n",
      "ExplainedVarNew: -0.822\n",
      "ExplainedVarOld: -0.759\n",
      "KL: -5e-08\n",
      "PolicyEntropy: 1.09e+03\n",
      "PolicyLoss: 1.14\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0226\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 500, Mean R = -61.9 *****\n",
      "ExplainedVarNew: -0.923\n",
      "ExplainedVarOld: -0.982\n",
      "KL: -5e-08\n",
      "PolicyEntropy: 1.04e+03\n",
      "PolicyLoss: 1.07\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0186\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 520, Mean R = -15.6 *****\n",
      "ExplainedVarNew: -0.721\n",
      "ExplainedVarOld: -0.847\n",
      "KL: -4.91e-08\n",
      "PolicyEntropy: 997\n",
      "PolicyLoss: 1.04\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0179\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 540, Mean R = 14.5 *****\n",
      "ExplainedVarNew: -0.747\n",
      "ExplainedVarOld: -0.582\n",
      "KL: -4.47e-08\n",
      "PolicyEntropy: 1e+03\n",
      "PolicyLoss: 1.05\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0216\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 560, Mean R = 21.1 *****\n",
      "ExplainedVarNew: -0.901\n",
      "ExplainedVarOld: -0.755\n",
      "KL: -4.26e-08\n",
      "PolicyEntropy: 964\n",
      "PolicyLoss: 1.02\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0231\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 580, Mean R = 37.6 *****\n",
      "ExplainedVarNew: -0.721\n",
      "ExplainedVarOld: -0.835\n",
      "KL: -4.2e-08\n",
      "PolicyEntropy: 941\n",
      "PolicyLoss: 1\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0218\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 600, Mean R = 145.4 *****\n",
      "ExplainedVarNew: -0.776\n",
      "ExplainedVarOld: -0.849\n",
      "KL: -4.12e-08\n",
      "PolicyEntropy: 858\n",
      "PolicyLoss: 0.925\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0216\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 620, Mean R = 228.7 *****\n",
      "ExplainedVarNew: -0.763\n",
      "ExplainedVarOld: -0.734\n",
      "KL: -4.07e-08\n",
      "PolicyEntropy: 818\n",
      "PolicyLoss: 0.863\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0208\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 640, Mean R = 261.2 *****\n",
      "ExplainedVarNew: -0.846\n",
      "ExplainedVarOld: -0.808\n",
      "KL: -4.04e-08\n",
      "PolicyEntropy: 736\n",
      "PolicyLoss: 0.793\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0203\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 660, Mean R = 303.5 *****\n",
      "ExplainedVarNew: -0.976\n",
      "ExplainedVarOld: -0.909\n",
      "KL: -3.92e-08\n",
      "PolicyEntropy: 686\n",
      "PolicyLoss: 0.768\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0196\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 680, Mean R = 315.6 *****\n",
      "ExplainedVarNew: -0.909\n",
      "ExplainedVarOld: -0.986\n",
      "KL: -3.85e-08\n",
      "PolicyEntropy: 656\n",
      "PolicyLoss: 0.732\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0184\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 700, Mean R = 329.7 *****\n",
      "ExplainedVarNew: -0.923\n",
      "ExplainedVarOld: -0.977\n",
      "KL: -3.76e-08\n",
      "PolicyEntropy: 585\n",
      "PolicyLoss: 0.681\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.017\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 720, Mean R = 357.6 *****\n",
      "ExplainedVarNew: -0.81\n",
      "ExplainedVarOld: -0.76\n",
      "KL: -3.61e-08\n",
      "PolicyEntropy: 527\n",
      "PolicyLoss: 0.608\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0196\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 740, Mean R = 374.9 *****\n",
      "ExplainedVarNew: -0.84\n",
      "ExplainedVarOld: -0.789\n",
      "KL: -3.39e-08\n",
      "PolicyEntropy: 477\n",
      "PolicyLoss: 0.557\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0206\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 760, Mean R = 391.9 *****\n",
      "ExplainedVarNew: -0.866\n",
      "ExplainedVarOld: -0.77\n",
      "KL: -3.2e-08\n",
      "PolicyEntropy: 443\n",
      "PolicyLoss: 0.516\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0226\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 780, Mean R = 403.1 *****\n",
      "ExplainedVarNew: -0.855\n",
      "ExplainedVarOld: -0.869\n",
      "KL: -3.09e-08\n",
      "PolicyEntropy: 383\n",
      "PolicyLoss: 0.463\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0221\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 800, Mean R = 411.5 *****\n",
      "ExplainedVarNew: -0.919\n",
      "ExplainedVarOld: -0.83\n",
      "KL: -2.98e-08\n",
      "PolicyEntropy: 347\n",
      "PolicyLoss: 0.419\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0237\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 820, Mean R = 435.3 *****\n",
      "ExplainedVarNew: -0.83\n",
      "ExplainedVarOld: -0.783\n",
      "KL: -2.74e-08\n",
      "PolicyEntropy: 309\n",
      "PolicyLoss: 0.393\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0263\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linesearch worked at  0\n",
      "***** Episode 840, Mean R = 469.1 *****\n",
      "ExplainedVarNew: -0.894\n",
      "ExplainedVarOld: -0.784\n",
      "KL: -2.48e-08\n",
      "PolicyEntropy: 270\n",
      "PolicyLoss: 0.353\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.029\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 860, Mean R = 471.7 *****\n",
      "ExplainedVarNew: -0.872\n",
      "ExplainedVarOld: -0.83\n",
      "KL: -2.3e-08\n",
      "PolicyEntropy: 261\n",
      "PolicyLoss: 0.33\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0311\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 880, Mean R = 491.8 *****\n",
      "ExplainedVarNew: -0.893\n",
      "ExplainedVarOld: -0.904\n",
      "KL: -2.14e-08\n",
      "PolicyEntropy: 266\n",
      "PolicyLoss: 0.338\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0299\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 900, Mean R = 515.3 *****\n",
      "ExplainedVarNew: -0.912\n",
      "ExplainedVarOld: -0.838\n",
      "KL: -1.97e-08\n",
      "PolicyEntropy: 248\n",
      "PolicyLoss: 0.318\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0323\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 920, Mean R = 522.8 *****\n",
      "ExplainedVarNew: -0.953\n",
      "ExplainedVarOld: -0.887\n",
      "KL: -1.88e-08\n",
      "PolicyEntropy: 247\n",
      "PolicyLoss: 0.319\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0338\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 940, Mean R = 551.9 *****\n",
      "ExplainedVarNew: -0.89\n",
      "ExplainedVarOld: -0.828\n",
      "KL: -1.85e-08\n",
      "PolicyEntropy: 242\n",
      "PolicyLoss: 0.291\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0378\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 960, Mean R = 567.2 *****\n",
      "ExplainedVarNew: -0.979\n",
      "ExplainedVarOld: -0.881\n",
      "KL: -1.85e-08\n",
      "PolicyEntropy: 240\n",
      "PolicyLoss: 0.274\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0397\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 980, Mean R = 582.7 *****\n",
      "ExplainedVarNew: -0.943\n",
      "ExplainedVarOld: -0.92\n",
      "KL: -1.79e-08\n",
      "PolicyEntropy: 212\n",
      "PolicyLoss: 0.244\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0418\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1000, Mean R = 607.2 *****\n",
      "ExplainedVarNew: -0.962\n",
      "ExplainedVarOld: -0.902\n",
      "KL: -1.79e-08\n",
      "PolicyEntropy: 211\n",
      "PolicyLoss: 0.225\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0438\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1020, Mean R = 630.1 *****\n",
      "ExplainedVarNew: -0.96\n",
      "ExplainedVarOld: -0.887\n",
      "KL: -1.79e-08\n",
      "PolicyEntropy: 188\n",
      "PolicyLoss: 0.188\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0477\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1040, Mean R = 626.7 *****\n",
      "ExplainedVarNew: -1.02\n",
      "ExplainedVarOld: -0.958\n",
      "KL: -1.78e-08\n",
      "PolicyEntropy: 180\n",
      "PolicyLoss: 0.18\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.049\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1060, Mean R = 623.5 *****\n",
      "ExplainedVarNew: -1.01\n",
      "ExplainedVarOld: -1.02\n",
      "KL: -1.79e-08\n",
      "PolicyEntropy: 154\n",
      "PolicyLoss: 0.154\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0489\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1080, Mean R = 628.4 *****\n",
      "ExplainedVarNew: -1.01\n",
      "ExplainedVarOld: -1\n",
      "KL: -1.77e-08\n",
      "PolicyEntropy: 143\n",
      "PolicyLoss: 0.143\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0493\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1100, Mean R = 629.3 *****\n",
      "ExplainedVarNew: -0.985\n",
      "ExplainedVarOld: -1.01\n",
      "KL: -1.77e-08\n",
      "PolicyEntropy: 150\n",
      "PolicyLoss: 0.15\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0486\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1120, Mean R = 631.7 *****\n",
      "ExplainedVarNew: -1\n",
      "ExplainedVarOld: -0.993\n",
      "KL: -1.78e-08\n",
      "PolicyEntropy: 189\n",
      "PolicyLoss: 0.189\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0487\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1140, Mean R = 625.5 *****\n",
      "ExplainedVarNew: -0.996\n",
      "ExplainedVarOld: -0.987\n",
      "KL: -1.8e-08\n",
      "PolicyEntropy: 226\n",
      "PolicyLoss: 0.226\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0496\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1160, Mean R = 629.6 *****\n",
      "ExplainedVarNew: -1.01\n",
      "ExplainedVarOld: -1.02\n",
      "KL: -1.79e-08\n",
      "PolicyEntropy: 195\n",
      "PolicyLoss: 0.195\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0487\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1180, Mean R = 631.8 *****\n",
      "ExplainedVarNew: -0.96\n",
      "ExplainedVarOld: -0.99\n",
      "KL: -1.78e-08\n",
      "PolicyEntropy: 211\n",
      "PolicyLoss: 0.211\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0485\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1200, Mean R = 630.2 *****\n",
      "ExplainedVarNew: -0.972\n",
      "ExplainedVarOld: -0.955\n",
      "KL: -1.78e-08\n",
      "PolicyEntropy: 202\n",
      "PolicyLoss: 0.202\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0491\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1220, Mean R = 635.7 *****\n",
      "ExplainedVarNew: -0.985\n",
      "ExplainedVarOld: -0.971\n",
      "KL: -1.77e-08\n",
      "PolicyEntropy: 164\n",
      "PolicyLoss: 0.164\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0493\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1240, Mean R = 634.1 *****\n",
      "ExplainedVarNew: -0.985\n",
      "ExplainedVarOld: -0.993\n",
      "KL: -1.76e-08\n",
      "PolicyEntropy: 212\n",
      "PolicyLoss: 0.212\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.049\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1260, Mean R = 622.9 *****\n",
      "ExplainedVarNew: -0.994\n",
      "ExplainedVarOld: -0.979\n",
      "KL: -1.76e-08\n",
      "PolicyEntropy: 217\n",
      "PolicyLoss: 0.217\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0494\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1280, Mean R = 622.2 *****\n",
      "ExplainedVarNew: -0.981\n",
      "ExplainedVarOld: -0.919\n",
      "KL: -1.75e-08\n",
      "PolicyEntropy: 243\n",
      "PolicyLoss: 0.243\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0531\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1300, Mean R = 635.1 *****\n",
      "ExplainedVarNew: -1.03\n",
      "ExplainedVarOld: -1.07\n",
      "KL: -1.74e-08\n",
      "PolicyEntropy: 253\n",
      "PolicyLoss: 0.253\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.05\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1320, Mean R = 630.7 *****\n",
      "ExplainedVarNew: -0.953\n",
      "ExplainedVarOld: -1.02\n",
      "KL: -1.75e-08\n",
      "PolicyEntropy: 228\n",
      "PolicyLoss: 0.228\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0483\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1340, Mean R = 630.3 *****\n",
      "ExplainedVarNew: -0.966\n",
      "ExplainedVarOld: -0.961\n",
      "KL: -1.76e-08\n",
      "PolicyEntropy: 232\n",
      "PolicyLoss: 0.232\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0482\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1360, Mean R = 604.0 *****\n",
      "ExplainedVarNew: -0.95\n",
      "ExplainedVarOld: -0.863\n",
      "KL: -1.74e-08\n",
      "PolicyEntropy: 237\n",
      "PolicyLoss: 0.237\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0535\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1380, Mean R = 627.6 *****\n",
      "ExplainedVarNew: -1.02\n",
      "ExplainedVarOld: -0.996\n",
      "KL: -1.72e-08\n",
      "PolicyEntropy: 260\n",
      "PolicyLoss: 0.26\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0528\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1400, Mean R = 616.5 *****\n",
      "ExplainedVarNew: -0.955\n",
      "ExplainedVarOld: -0.976\n",
      "KL: -1.72e-08\n",
      "PolicyEntropy: 265\n",
      "PolicyLoss: 0.265\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0536\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1420, Mean R = 626.0 *****\n",
      "ExplainedVarNew: -0.99\n",
      "ExplainedVarOld: -1.01\n",
      "KL: -1.75e-08\n",
      "PolicyEntropy: 275\n",
      "PolicyLoss: 0.275\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0516\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1440, Mean R = 602.8 *****\n",
      "ExplainedVarNew: -1.03\n",
      "ExplainedVarOld: -1.07\n",
      "KL: -1.73e-08\n",
      "PolicyEntropy: 253\n",
      "PolicyLoss: 0.253\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0492\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1460, Mean R = 630.2 *****\n",
      "ExplainedVarNew: -0.991\n",
      "ExplainedVarOld: -1.01\n",
      "KL: -1.74e-08\n",
      "PolicyEntropy: 245\n",
      "PolicyLoss: 0.245\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0494\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1480, Mean R = 629.2 *****\n",
      "ExplainedVarNew: -0.975\n",
      "ExplainedVarOld: -0.962\n",
      "KL: -1.75e-08\n",
      "PolicyEntropy: 276\n",
      "PolicyLoss: 0.276\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0503\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1500, Mean R = 616.6 *****\n",
      "ExplainedVarNew: -1.02\n",
      "ExplainedVarOld: -0.998\n",
      "KL: -1.74e-08\n",
      "PolicyEntropy: 258\n",
      "PolicyLoss: 0.258\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0503\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1520, Mean R = 597.1 *****\n",
      "ExplainedVarNew: -1.09\n",
      "ExplainedVarOld: -1.13\n",
      "KL: -1.74e-08\n",
      "PolicyEntropy: 243\n",
      "PolicyLoss: 0.243\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0467\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n",
      "***** Episode 1540, Mean R = 596.0 *****\n",
      "ExplainedVarNew: -0.947\n",
      "ExplainedVarOld: -1.01\n",
      "KL: -1.74e-08\n",
      "PolicyEntropy: 227\n",
      "PolicyLoss: 0.227\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.047\n",
      "\n",
      "\n",
      "Linesearch worked at  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-02ef94536a7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlog_batch_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_sum_rew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mvalue_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_sum_rew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnameDirLogWeights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# write logger results to file and stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code_delivery/value.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, logger, episode, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalueModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mold_exp_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer_x\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvar\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3621\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3622\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# Note that x may not be inexact and that we need it to be an array,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# not a scalar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0marrmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 3000\n",
    "gamma = 0.995 \n",
    "batch_size = 20\n",
    "hid1_size = 8\n",
    "\n",
    "# Initialize gym environment\n",
    "env_name = \"HalfCheetah-v2\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim += 1  # add 1 to obs dimension for time step feature (see run_episode())\n",
    "\n",
    "# Initialize some global variables\n",
    "now = datetime.now().strftime(\"%b-%d_%H:%M:%S\")  # create unique directories for logs\n",
    "logger = Logger(logname=env_name, now=now) # Training log\n",
    "nameDirLogWeights = f\"log-weights/TRPO-HalfCheetah-v2-{now}\" # Weights log\n",
    "aigym_path = os.path.join('/tmp', env_name, now)\n",
    "env = wrappers.Monitor(env, aigym_path, force=True)\n",
    "scaler = Scaler(obs_dim)\n",
    "value_function = NNValueFunction(obs_dim, hid1_size)\n",
    "policy = Policy(obs_dim, act_dim, hid1_size)\n",
    "\n",
    "# Run a few episodes of untrained policy to initialize scaler:\n",
    "run_policy(env, policy, scaler, logger, episodes=5)\n",
    "\n",
    "episode = 0\n",
    "while episode < num_episodes:\n",
    "    # run batch_size episodes and collect trajectories\n",
    "    trajectories, entropy = run_policy(env, policy, scaler, logger, episodes=batch_size)\n",
    "    episode += len(trajectories)\n",
    "    add_value(trajectories, value_function)  # compute and add estimated values to episodes\n",
    "    # compute and add discount sum rewards and advantages to trajectories\n",
    "    add_discount_sum_reward_and_advantage(trajectories, gamma) \n",
    "    # concatenate all episodes into single Numpy arrays\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    disc_sum_rew = np.concatenate([t['disc_sum_rew'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "    # normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "    # add various stats to training log\n",
    "    log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n",
    "    policy.update(observes, actions, advantages, logger, entropy, env)  # update policy\n",
    "    value_function.fit(observes, disc_sum_rew, logger, episode, nameDirLogWeights)  # update value function\n",
    "    logger.write(display=True)  # write logger results to file and stdout\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 6.11046553e-01, -5.06131113e-01, -1.37420461e-01, ...,\n",
       "         -4.54428606e-02, -4.02506962e-02,  0.00000000e+00],\n",
       "        [ 5.46202421e-01, -6.13432646e-01,  6.17069378e-02, ...,\n",
       "          1.77567685e+00,  1.20282304e+00,  1.00000005e-03],\n",
       "        [ 3.95448714e-01, -7.19012439e-01,  3.75297546e-01, ...,\n",
       "          1.23307860e+00,  1.04113460e+00,  2.00000009e-03],\n",
       "        ...,\n",
       "        [ 2.00358592e-02, -1.14863664e-01,  2.78079838e-01, ...,\n",
       "          1.66731268e-01, -3.92918177e-02,  9.96999979e-01],\n",
       "        [ 1.29601276e-02, -9.64251384e-02,  2.69828349e-01, ...,\n",
       "         -5.16755246e-02,  1.05855510e-01,  9.98000026e-01],\n",
       "        [ 1.06102169e-01,  3.75017673e-02, -1.37826040e-01, ...,\n",
       "         -1.95069090e-01, -8.95883888e-02,  9.99000013e-01]], dtype=float32),\n",
       " [2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " array([ 4.63031560e-01,  8.04220319e-01, -2.93021828e-01, -2.60460734e-01,\n",
       "         8.37225437e-01,  4.52251047e-01,  4.44947422e-01,  6.78407431e-01,\n",
       "         8.36870015e-01,  6.66083574e-01,  6.86338246e-01,  9.51932669e-01,\n",
       "         1.24250412e+00,  1.10666275e+00,  1.14974213e+00,  4.56456900e-01,\n",
       "         9.30420995e-01,  1.02187586e+00,  9.28388059e-01,  9.14687514e-01,\n",
       "         5.91311276e-01,  1.00824118e+00,  1.08327293e+00,  6.31687105e-01,\n",
       "         5.08504331e-01,  2.06502140e-01,  7.43633091e-01,  9.10727978e-01,\n",
       "         7.10767806e-01,  6.92685962e-01,  3.68946344e-01,  8.94878983e-01,\n",
       "         1.10454476e+00,  7.26212084e-01,  5.60663283e-01,  3.16005290e-01,\n",
       "         8.80358279e-01,  1.06702566e+00,  6.22154891e-01,  4.84220684e-01,\n",
       "         2.86868453e-01,  8.47179115e-01,  9.62552190e-01,  6.91911697e-01,\n",
       "         6.05073035e-01,  2.76241571e-01,  7.70326495e-01,  9.06825960e-01,\n",
       "         7.87940979e-01,  9.01221693e-01,  5.23133337e-01,  1.00981200e+00,\n",
       "         1.08898962e+00,  5.46789408e-01,  3.65837514e-01,  1.30411714e-01,\n",
       "         6.89996183e-01,  8.53638649e-01,  6.76041961e-01,  7.08431542e-01,\n",
       "         3.96206945e-01,  9.25110936e-01,  1.13707137e+00,  6.82540894e-01,\n",
       "         5.13111651e-01,  3.08750391e-01,  8.83627415e-01,  1.04979455e+00,\n",
       "         5.89070082e-01,  4.42724854e-01,  2.67972618e-01,  8.24386835e-01,\n",
       "         9.08042014e-01,  6.85706913e-01,  6.39171124e-01,  2.45458692e-01,\n",
       "         7.29168236e-01,  8.70553851e-01,  7.52697408e-01,  8.24991822e-01,\n",
       "         5.03841519e-01,  9.58880603e-01,  9.85963285e-01,  3.66184682e-01,\n",
       "         1.35806978e-01, -1.91171914e-01,  4.14526403e-01,  7.00464785e-01,\n",
       "         3.54700983e-01,  3.56878400e-01,  8.18597972e-02,  7.99049973e-01,\n",
       "         1.18087554e+00,  6.99811101e-01,  5.24506569e-01,  3.77565056e-01,\n",
       "         1.13561463e+00,  1.09381330e+00,  8.13630044e-01,  8.29516649e-01,\n",
       "         8.09262753e-01,  1.22713590e+00,  1.14107192e+00,  9.58039165e-01,\n",
       "         6.38173938e-01,  2.21209362e-01,  6.85840905e-01,  7.74827182e-01,\n",
       "         6.72269285e-01,  8.95897865e-01,  5.28143704e-01,  8.91564548e-01,\n",
       "         9.72361922e-01,  6.84335113e-01,  4.58057880e-01,  1.09700806e-01,\n",
       "         6.24422073e-01,  8.40679049e-01,  5.65665066e-01,  4.82087374e-01,\n",
       "         3.27520967e-01,  8.85186732e-01,  7.42847025e-01,  2.98672229e-01,\n",
       "         8.14467892e-02, -1.04241759e-01,  5.53203046e-01,  8.18697512e-01,\n",
       "         4.78745848e-01,  3.89019519e-01,  2.26830244e-01,  9.14067388e-01,\n",
       "         1.16047788e+00,  7.99218893e-01,  7.35715151e-01,  5.72135031e-01,\n",
       "         1.19426823e+00,  1.17090869e+00,  4.76207495e-01,  2.13229939e-01,\n",
       "         1.82378739e-02,  6.59646332e-01,  8.86195362e-01,  5.64705551e-01,\n",
       "         6.40733182e-01,  3.74120474e-01,  1.02255058e+00,  1.23949778e+00,\n",
       "         9.31601405e-01,  9.45509613e-01,  6.89940929e-01,  1.27546978e+00,\n",
       "         9.09642816e-01,  4.54064697e-01,  1.48515552e-01, -1.28515661e-01,\n",
       "         5.29602170e-01,  8.12341273e-01,  4.04278785e-01,  3.22808832e-01,\n",
       "         1.23117305e-01,  8.45305085e-01,  1.15244150e+00,  6.92039669e-01,\n",
       "         5.16431808e-01,  3.96073073e-01,  1.10614669e+00,  1.36369395e+00,\n",
       "         6.02785766e-01,  3.30428809e-01,  3.50519806e-01,  1.01145661e+00,\n",
       "         9.56306517e-01,  4.87293094e-01,  4.67451394e-01,  3.51502359e-01,\n",
       "         9.00753617e-01,  7.47102141e-01,  7.30741084e-01,  7.92527199e-01,\n",
       "         2.76951909e-01,  6.71317637e-01,  7.10358799e-01,  7.46624947e-01,\n",
       "         5.99901259e-01,  2.86617652e-02,  4.92543072e-01,  7.09182382e-01,\n",
       "         5.55121303e-01,  5.89305878e-01,  2.63413876e-01,  8.87945294e-01,\n",
       "         1.21468723e+00,  4.92301464e-01,  2.27859914e-01,  1.83296949e-01,\n",
       "         8.49614918e-01,  1.02947271e+00,  4.21316952e-01,  2.64451772e-01,\n",
       "         2.68298060e-01,  9.04178500e-01,  8.00077736e-01,  7.11236477e-01,\n",
       "         6.53166831e-01,  2.58440137e-01,  7.15443015e-01,  7.60314286e-01,\n",
       "         7.22381115e-01,  8.55790377e-01,  3.13689977e-01,  7.45011985e-01,\n",
       "         8.95206153e-01,  8.05575728e-01,  7.31567919e-01,  4.19970304e-01,\n",
       "         9.07121718e-01,  8.25601578e-01,  3.20437312e-01,  1.33802379e-02,\n",
       "        -2.52667397e-01,  4.30319190e-01,  8.12424660e-01,  3.40892613e-01,\n",
       "         1.35104060e-01,  2.68242478e-01,  9.48590398e-01,  6.31434202e-01,\n",
       "         3.10240984e-01,  4.51807529e-01,  6.10456407e-01,  5.40065765e-01,\n",
       "         4.53968197e-01,  7.79645979e-01,  1.15390575e+00,  9.63995516e-01,\n",
       "         9.61139143e-01,  8.65459085e-01,  9.79220808e-01,  1.15402114e+00,\n",
       "         8.40505302e-01,  6.75152540e-01,  3.57897907e-01,  8.65225375e-01,\n",
       "         9.67068791e-01,  8.58349681e-01,  8.99654925e-01,  4.11520153e-01,\n",
       "         9.15127635e-01,  1.07579601e+00,  8.59392762e-01,  6.34817600e-01,\n",
       "         3.46090734e-01,  8.82484019e-01,  1.02874064e+00,  5.19422889e-01,\n",
       "         4.42255616e-01,  2.16453999e-01,  7.45167494e-01,  8.79723430e-01,\n",
       "         6.42900229e-01,  5.46388507e-01,  2.71909416e-01,  4.86268312e-01,\n",
       "         9.35545564e-01,  7.53524482e-01,  7.11295605e-01,  2.90315986e-01,\n",
       "         8.05721402e-01,  1.03803110e+00,  7.25629032e-01,  5.83104312e-01,\n",
       "         4.16502267e-01,  9.73136485e-01,  8.81460845e-01,  4.54875112e-01,\n",
       "         2.79707462e-01,  1.05137751e-01,  4.28257197e-01,  9.24160659e-01,\n",
       "         6.91598237e-01,  7.29492426e-01,  5.37319005e-01,  1.11692977e+00,\n",
       "         1.06199133e+00,  6.61417723e-01,  5.14081359e-01,  3.22251499e-01,\n",
       "         8.58943939e-01,  8.94359469e-01,  4.88828957e-01,  5.28585076e-01,\n",
       "         2.21618176e-01,  7.20870733e-01,  8.10438514e-01,  7.44613647e-01,\n",
       "         6.57497108e-01,  2.14065552e-01,  6.96655095e-01,  8.81067038e-01,\n",
       "         6.91185117e-01,  6.11120343e-01,  3.22355598e-01,  8.52075160e-01,\n",
       "         1.08719730e+00,  6.19964361e-01,  3.98106813e-01,  3.00429642e-01,\n",
       "         9.02845025e-01,  8.35343421e-01,  4.30266738e-01,  3.84424090e-01,\n",
       "         2.46268034e-01,  8.16292405e-01,  8.53410602e-01,  5.77704012e-01,\n",
       "         5.23188293e-01,  1.26345292e-01,  2.79206067e-01,  6.88808084e-01,\n",
       "         5.68402290e-01,  7.87034035e-01,  4.82968152e-01,  8.94789338e-01,\n",
       "         8.01013052e-01,  6.61065459e-01,  4.89503771e-01,  2.42747381e-01,\n",
       "         8.27161014e-01,  8.36706281e-01,  1.98354021e-01, -2.92532071e-02,\n",
       "        -2.69460440e-01,  1.07648991e-01,  7.74461150e-01,  3.07935774e-01,\n",
       "         9.64871496e-02,  2.27310345e-01,  8.93379033e-01,  5.32480776e-01,\n",
       "         1.31627753e-01,  4.64999110e-01,  5.24090469e-01,  4.87255126e-01,\n",
       "         4.38472807e-01,  7.22277582e-01,  1.06504357e+00,  9.10599828e-01,\n",
       "         8.68658602e-01,  5.58061838e-01,  9.78446722e-01,  1.02353704e+00,\n",
       "         7.71504939e-01,  6.36938870e-01,  2.06246033e-01,  3.59625340e-01,\n",
       "         7.62601018e-01,  6.40379846e-01,  8.94346297e-01,  5.62468767e-01,\n",
       "         8.46373558e-01,  9.60247874e-01,  7.55044460e-01,  5.99896729e-01,\n",
       "         3.06035697e-01,  8.50036323e-01,  1.05247355e+00,  5.09326279e-01,\n",
       "         2.58273751e-01,  1.34662718e-01,  4.59778816e-01,  9.22288954e-01,\n",
       "         6.28469884e-01,  6.16159976e-01,  4.30516601e-01,  9.63308811e-01,\n",
       "         1.08250892e+00,  6.90364778e-01,  5.50554752e-01,  2.88741797e-01,\n",
       "         4.95211154e-01,  9.13191736e-01,  7.55610526e-01,  6.89893901e-01,\n",
       "         2.97918975e-01,  7.91187763e-01,  9.50688004e-01,  7.66202331e-01,\n",
       "         6.29172146e-01,  3.50605041e-01,  8.91018569e-01,  1.05839622e+00,\n",
       "         4.76216406e-01,  3.82895976e-01,  2.10177854e-01,  4.70400512e-01,\n",
       "         8.85494232e-01,  5.01645803e-01,  3.76861423e-01,  1.40179947e-01,\n",
       "         3.47083420e-01,  7.75112987e-01,  6.67521298e-01,  8.09622645e-01,\n",
       "         5.20426393e-01,  9.79254484e-01,  1.02828658e+00,  5.21702349e-01,\n",
       "         3.38245660e-01,  8.66399705e-02,  6.57917082e-01,  8.46747577e-01,\n",
       "         6.43519521e-01,  6.60158873e-01,  4.75502998e-01,  9.32725251e-01,\n",
       "         1.03933001e+00,  5.01795173e-01,  3.26456606e-01,  1.59245759e-01,\n",
       "         4.53915477e-01,  9.00396705e-01,  5.97286403e-01,  5.45732260e-01,\n",
       "         2.88595408e-01,  8.10704947e-01,  9.48485136e-01,  7.68857896e-01,\n",
       "         7.38681436e-01,  3.59178543e-01,  8.75917196e-01,  1.04894114e+00,\n",
       "         8.21049333e-01,  7.37738431e-01,  3.98964524e-01,  9.28291023e-01,\n",
       "         1.10275614e+00,  6.84719503e-01,  4.37164307e-01,  2.38881826e-01,\n",
       "         7.87349761e-01,  9.11332965e-01,  6.03140831e-01,  4.87706244e-01,\n",
       "         2.22685352e-01,  4.30973232e-01,  8.68640840e-01,  7.05886483e-01,\n",
       "         6.56664789e-01,  2.62445629e-01,  7.67532170e-01,  9.73378122e-01,\n",
       "         7.36346602e-01,  6.59582078e-01,  4.60325569e-01,  9.64946330e-01,\n",
       "         9.86155093e-01,  4.97971624e-01,  2.95733690e-01,  1.58570156e-01,\n",
       "         4.71676052e-01,  9.09582257e-01,  6.06208980e-01,  5.89357972e-01,\n",
       "         3.08021903e-01,  5.18467546e-01,  9.64431167e-01,  7.64622688e-01,\n",
       "         6.81736588e-01,  3.34314138e-01,  8.34569573e-01,  1.01379287e+00,\n",
       "         7.78745115e-01,  6.95168436e-01,  3.95895064e-01,  9.39031303e-01,\n",
       "         1.13731468e+00,  6.21040463e-01,  4.29167747e-01,  2.60977536e-01,\n",
       "         5.48077941e-01,  9.81328428e-01,  6.65998995e-01,  6.43787384e-01,\n",
       "         3.33902150e-01,  5.39532602e-01,  9.67798948e-01,  7.82583416e-01,\n",
       "         7.47086108e-01,  2.88765758e-01,  7.82345831e-01,  9.96511102e-01,\n",
       "         7.50752151e-01,  6.14254653e-01,  4.12602931e-01,  1.00948882e+00,\n",
       "         7.84896433e-01,  2.46812239e-01, -2.89043747e-02, -6.51326329e-02,\n",
       "         4.61100429e-01,  2.58702993e-01, -1.96858607e-02,  1.07938640e-01,\n",
       "         6.24905229e-01,  4.05889690e-01,  1.88437015e-01,  4.22902316e-01,\n",
       "         4.51287389e-01,  4.90583867e-01,  4.90827262e-01,  6.49030328e-01,\n",
       "         9.90582705e-01,  5.83634377e-01,  6.55763865e-01,  7.42065787e-01,\n",
       "         5.31926155e-01,  7.14274764e-01,  7.67657399e-01,  6.86894536e-01,\n",
       "         3.17267179e-01,  5.58351040e-01,  6.35193527e-01,  3.28736693e-01,\n",
       "         1.87434509e-01, -7.18060136e-02,  4.88744378e-01,  6.95960283e-01,\n",
       "         4.37847584e-01,  3.51088792e-01,  2.19443321e-01,  8.41434419e-01,\n",
       "         8.01334798e-01,  3.07792783e-01,  1.17131718e-01, -2.97687668e-02,\n",
       "         3.29485863e-01,  8.53747249e-01,  5.48646927e-01,  5.09741247e-01,\n",
       "         3.36728334e-01,  9.83736753e-01,  1.13738739e+00,  8.44915986e-01,\n",
       "         7.48305142e-01,  3.96301180e-01,  9.23226953e-01,  1.12461662e+00,\n",
       "         7.44500577e-01,  5.81941485e-01,  3.73989999e-01,  9.26903844e-01,\n",
       "         1.00987840e+00,  3.53126079e-01,  2.16956913e-01, -4.30605985e-04,\n",
       "         3.06471288e-01,  8.09620559e-01,  5.33201933e-01,  5.09656012e-01,\n",
       "         3.53405476e-01,  9.65850770e-01,  1.07078278e+00,  3.45391035e-01,\n",
       "         2.39763796e-01,  4.58469465e-02,  3.06760520e-01,  7.39728332e-01,\n",
       "         5.94043791e-01,  6.12464190e-01,  2.98339218e-01,  8.12444568e-01,\n",
       "         9.97565806e-01,  6.46681726e-01,  4.92813528e-01,  2.88502336e-01,\n",
       "         5.51381767e-01,  1.01143122e+00,  3.52454036e-01,  2.92319119e-01,\n",
       "         8.73755515e-02,  3.72500986e-01,  8.48571479e-01,  6.41074657e-01,\n",
       "         6.42924249e-01,  4.76163059e-01,  9.00879323e-01,  9.77480710e-01,\n",
       "         4.59522069e-01,  3.20370466e-01,  1.13306254e-01,  6.72878861e-01,\n",
       "         7.94869244e-01,  6.45196378e-01,  6.24028921e-01,  2.81495154e-01,\n",
       "         8.02844942e-01,  9.87400293e-01,  7.51754105e-01,  7.09616482e-01,\n",
       "         3.90667498e-01,  9.22928154e-01,  1.12317038e+00,  6.74293280e-01,\n",
       "         4.96739924e-01,  2.92314976e-01,  5.59524000e-01,  1.00318182e+00,\n",
       "         6.84611619e-01,  5.51602066e-01,  3.00788611e-01,  5.29815376e-01,\n",
       "         9.36355770e-01,  7.52750874e-01,  6.05171680e-01,  2.66746730e-01,\n",
       "         4.59914088e-01,  8.82538795e-01,  7.80469716e-01,  8.20777297e-01,\n",
       "         3.46521914e-01,  8.47958207e-01,  1.04226005e+00,  7.56509602e-01,\n",
       "         5.99996686e-01,  3.64765763e-01,  6.12579823e-01,  9.68442380e-01,\n",
       "         4.67721909e-01,  3.46442491e-01,  1.61914557e-01,  4.52489167e-01,\n",
       "         9.00711238e-01,  6.98557854e-01,  7.08823144e-01,  3.68173182e-01,\n",
       "         8.94179702e-01,  1.06061351e+00,  7.13480353e-01,  5.85972309e-01,\n",
       "         3.43858838e-01,  5.74282229e-01,  9.92967010e-01,  5.72352231e-01,\n",
       "         4.59361911e-01,  1.83058321e-01,  3.81121129e-01,  8.19283664e-01,\n",
       "         7.06751764e-01,  8.45107794e-01,  5.34412324e-01,  9.97990787e-01,\n",
       "         9.73608375e-01,  4.62520421e-01,  2.46551871e-01, -2.61186175e-02,\n",
       "         5.67534208e-01,  7.95261264e-01,  5.06292820e-01,  4.51128036e-01,\n",
       "         3.07755888e-01,  9.41277385e-01,  1.10035014e+00,  2.43362769e-01,\n",
       "         7.38716349e-02, -1.56505987e-01,  1.69822514e-01,  7.44462430e-01,\n",
       "         3.86345118e-01,  2.53639042e-01,  8.68733451e-02,  8.00565481e-01,\n",
       "         1.07926106e+00,  6.88502610e-01,  6.15090966e-01,  4.52029824e-01,\n",
       "         1.13043797e+00,  1.35201657e+00,  6.48801088e-01,  3.74751180e-01,\n",
       "         2.76992381e-01,  6.01494014e-01,  1.05036855e+00,  7.29367912e-01,\n",
       "         7.25841582e-01,  5.00064075e-01,  1.00066221e+00,  9.97178614e-01,\n",
       "         7.82054424e-01,  6.27789140e-01,  2.10413128e-01,  3.77718151e-01,\n",
       "         8.00879240e-01,  7.05116868e-01,  7.83728361e-01,  4.62221652e-01,\n",
       "         9.12543178e-01,  9.27940905e-01,  3.71555299e-01,  1.34777710e-01,\n",
       "        -2.25310415e-01,  8.97609219e-02,  7.09073544e-01,  3.26874644e-01,\n",
       "         3.10577899e-01,  3.46177304e-03,  7.52451301e-01,  1.20110726e+00,\n",
       "         6.21893287e-01,  3.17873269e-01,  4.33774203e-01,  8.91456306e-01,\n",
       "         7.70365715e-01,  5.77113211e-01,  8.36736798e-01,  9.18953478e-01,\n",
       "         1.06510425e+00,  9.56460118e-01,  8.64513755e-01,  5.98385930e-01,\n",
       "         6.55413449e-01,  8.17873180e-01,  5.83774388e-01,  1.94257796e-01,\n",
       "         5.60173869e-01,  5.88218987e-01,  1.25046968e-01, -4.51697633e-02,\n",
       "        -3.00032258e-01,  3.37702967e-02,  6.51913464e-01,  3.04551601e-01,\n",
       "         1.17144890e-01, -2.27755755e-02,  7.30686665e-01,  1.13141668e+00,\n",
       "         5.51256835e-01,  2.32253030e-01,  5.15349507e-01,  9.40095961e-01,\n",
       "         7.25588679e-01,  8.11351359e-01,  8.86460245e-01,  7.43134439e-01,\n",
       "         9.11207020e-01,  1.04349792e+00,  1.11277151e+00,  7.91155338e-01,\n",
       "         9.19084489e-01,  1.09700954e+00,  6.06898546e-01,  2.11469248e-01,\n",
       "         6.06590092e-01,  6.54832184e-01,  1.92139223e-01,  4.76148129e-02,\n",
       "        -2.27050334e-01,  1.08344197e-01,  7.51480460e-01,  3.57268333e-01,\n",
       "         1.33688807e-01, -1.57981720e-02,  7.46715844e-01,  1.16113877e+00,\n",
       "         5.11749268e-01,  1.75693169e-01,  4.83578324e-01,  9.17841256e-01,\n",
       "         7.10997462e-01,  6.83437645e-01,  7.79747903e-01,  6.81236148e-01,\n",
       "         8.18645954e-01,  9.79416192e-01,  8.19395423e-01,  5.58653414e-01,\n",
       "         6.33252084e-01,  7.88964331e-01,  5.18543303e-01,  6.66525066e-02,\n",
       "         4.40314680e-01,  5.97639680e-01,  3.88546854e-01,  2.64769107e-01,\n",
       "         5.55931665e-02,  3.62319648e-01,  9.17323112e-01,  3.78692895e-01,\n",
       "         1.52373835e-01,  8.22805762e-02,  4.86668557e-01,  1.00976527e+00,\n",
       "         5.20140231e-01,  4.07871485e-01,  3.60674858e-01,  1.00684714e+00,\n",
       "         8.58652771e-01,  4.04074579e-01,  3.57209325e-01,  1.34898797e-01,\n",
       "         3.92706245e-01,  7.65960038e-01,  6.18181646e-01,  9.24940050e-01,\n",
       "         5.25767267e-01,  9.72391665e-01,  1.05413878e+00,  6.32871807e-01,\n",
       "         4.85722691e-01,  2.25290239e-01,  4.33343053e-01,  8.24905634e-01,\n",
       "         5.50274253e-01,  5.37089109e-01,  1.73765406e-01,  3.77792567e-01,\n",
       "         8.82014513e-01,  6.79397166e-01,  6.45710349e-01,  4.41579551e-01,\n",
       "         9.62143004e-01,  9.09091175e-01,  4.41706032e-01,  2.43335456e-01,\n",
       "         3.85826863e-02,  3.66493881e-01,  9.01722968e-01,  6.41033888e-01,\n",
       "         6.52638078e-01,  4.44096863e-01,  1.06987667e+00,  1.27656484e+00,\n",
       "         7.33183920e-01,  4.95407313e-01,  3.24158639e-01,  6.00792348e-01,\n",
       "         1.02180958e+00,  6.98805630e-01,  5.90688944e-01,  3.05230349e-01,\n",
       "         5.09484768e-01,  9.07801211e-01,  8.14051151e-01,  7.45496213e-01,\n",
       "         1.93209544e-01,  3.90871197e-01,  9.08308685e-01,  7.21163332e-01,\n",
       "         7.42113650e-01,  4.13956434e-01,  1.01265848e+00,  1.25803864e+00,\n",
       "         6.72075868e-01,  3.98533940e-01,  2.28784993e-01,  5.32967031e-01,\n",
       "         1.03138053e+00,  5.73838770e-01,  4.97087777e-01,  3.29645038e-01,\n",
       "         5.66242278e-01,  1.01601374e+00,  6.93364859e-01,  6.01624608e-01,\n",
       "         3.30622464e-01,  5.51219702e-01,  9.82471406e-01,  7.98176348e-01,\n",
       "         7.39210367e-01,  3.54423374e-01,  5.67160368e-01,  1.04160082e+00,\n",
       "         8.22352290e-01,  7.67403483e-01,  3.99674237e-01,  6.21205986e-01,\n",
       "         1.13128650e+00,  7.50826120e-01,  5.90680659e-01,  3.92041802e-01,\n",
       "         6.48281217e-01,  8.61208022e-01,  3.95025462e-01,  2.43295938e-01,\n",
       "         1.61209721e-02,  3.21837485e-01,  8.04151952e-01,  5.68387508e-01,\n",
       "         6.45380259e-01,  4.31069404e-01,  1.01779532e+00,  1.13830471e+00,\n",
       "         5.61276078e-01,  2.98126787e-01,  9.16604847e-02,  3.37471843e-01,\n",
       "         7.76053607e-01,  6.18866920e-01,  6.42137229e-01,  3.65688443e-01,\n",
       "         8.97027433e-01,  8.26816797e-01,  3.11305165e-01,  1.29111782e-01,\n",
       "        -1.48636937e-01,  1.55996352e-01,  6.99207366e-01,  4.07849163e-01,\n",
       "         4.61908758e-01,  2.18946770e-01,  8.83008778e-01,  1.17868912e+00,\n",
       "         7.38840163e-01,  5.23903370e-01,  4.21995789e-01,  1.07840645e+00,\n",
       "         1.09854412e+00,  3.14324647e-01,  7.08269477e-02, -3.92337218e-02,\n",
       "         3.28893542e-01,  8.30512345e-01,  5.02057135e-01,  5.80284715e-01,\n",
       "         5.87817132e-01,  1.01962483e+00,  9.33249414e-01,  7.96781242e-01,\n",
       "         6.12606645e-01,  1.10161638e+00,  6.44284248e-01,  2.62548149e-01,\n",
       "        -6.43176958e-02, -4.79244813e-02,  4.91483957e-01,  2.73741841e-01,\n",
       "        -4.27458063e-02,  1.08676322e-01,  6.48511410e-01,  3.98006290e-01,\n",
       "         1.86892733e-01,  3.67438793e-01,  4.83865023e-01,  4.87480611e-01,\n",
       "         4.48923320e-01,  5.90636671e-01,  1.20916414e+00,  1.00660133e+00,\n",
       "         6.46603167e-01,  6.55183375e-01,  4.04382527e-01,  6.03012979e-01,\n",
       "         7.07148194e-01,  7.80739665e-01,  5.23070455e-01,  8.69312108e-01,\n",
       "         6.72035873e-01,  7.61254489e-01,  4.08693820e-01, -4.75157686e-02,\n",
       "         1.61957785e-01,  7.34195590e-01,  4.27653193e-01,  2.49273777e-01,\n",
       "         1.08828738e-01,  7.88473904e-01,  1.03140223e+00,  3.37836593e-01,\n",
       "         1.29406661e-01,  1.17391326e-01,  5.20612061e-01,  1.01603687e+00,\n",
       "         5.63110113e-01,  4.91900265e-01,  4.38560516e-01,  9.33346629e-01,\n",
       "         8.85821164e-01,  6.85013175e-01,  6.37035728e-01,  3.22948903e-01,\n",
       "         5.17224789e-01,  9.15488780e-01,  8.33441854e-01,  7.86468685e-01,\n",
       "         2.81003743e-01,  4.79310066e-01,  9.75125194e-01,  7.55984724e-01,\n",
       "         6.62439108e-01,  3.97247523e-01,  6.42449737e-01,  9.79860306e-01,\n",
       "         4.30267960e-01,  1.90606564e-01, -1.64156649e-02,  3.22195947e-01,\n",
       "         8.75155926e-01,  5.77162743e-01,  5.65852165e-01,  3.53322029e-01],\n",
       "       dtype=float32),\n",
       " array([[ 1.1542588e-02,  7.4449845e-02,  4.1707274e-02, ...,\n",
       "         -5.7557661e-02, -8.9733854e-02,  0.0000000e+00],\n",
       "        [-2.2679571e-02, -3.4665380e-02,  2.8422603e-01, ...,\n",
       "          8.6757412e+00,  7.4740076e+00,  1.0000000e-03],\n",
       "        [-1.0224140e-01, -1.4202979e-01,  6.6615039e-01, ...,\n",
       "          6.0736756e+00,  6.4901805e+00,  2.0000001e-03],\n",
       "        ...,\n",
       "        [-3.0036938e-01,  4.7233081e-01,  5.4774821e-01, ...,\n",
       "          9.5993727e-01, -8.3899349e-02,  9.9699998e-01],\n",
       "        [-3.0410367e-01,  4.9108100e-01,  5.3769869e-01, ...,\n",
       "         -8.7446801e-02,  7.9927981e-01,  9.9800003e-01],\n",
       "        [-2.5494701e-01,  6.2727165e-01,  4.1213308e-02, ...,\n",
       "         -7.7510005e-01, -3.8993931e-01,  9.9900001e-01]], dtype=float32),\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=165.6693840741933>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run_episode(env, policy, scaler, animate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
