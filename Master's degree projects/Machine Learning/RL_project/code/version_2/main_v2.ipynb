{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from policy_v2 import Policy\n",
    "from value import NNValueFunction\n",
    "import scipy.signal\n",
    "from utils import Logger, Scaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single episode: it returns a scalar for the entropy\n",
    "# and a 4-tuple of NumPy arrays for observations, actions, rewards and unscaled observations \n",
    "def run_episode(env, policy, scaler, animate=False): # animate: if True uses env.render() to animate episode\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, unscaled_observes = [], [], [], []\n",
    "    done = False\n",
    "    step = 0.0\n",
    "    scale, offset = scaler.get()\n",
    "    scale[-1] = 1.0  # don't scale time step feature\n",
    "    offset[-1] = 0.0  # don't offset time step feature\n",
    "    while not done:\n",
    "        if animate:\n",
    "            env.render()\n",
    "        obs = np.concatenate([obs, [step]])  # add time step feature\n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        unscaled_observes.append(obs)\n",
    "        obs = np.float32((obs - offset) * scale)  # center and scale observations\n",
    "        observes.append(obs)\n",
    "        action = policy.sample(obs)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, _ = env.step(action.flatten())\n",
    "        rewards.append(reward)\n",
    "        step += 1e-3  # increment time step feature\n",
    "\n",
    "    return (np.concatenate(observes), # observes: shape = (episode len, obs_dim)\n",
    "            np.concatenate(actions), # actions: shape = (episode len, act_dim)\n",
    "            np.array(rewards, dtype=np.float32), #rewards: shape = (episode len,)\n",
    "            np.concatenate(unscaled_observes)) # unscaled_observes: useful for training scaler, shape = (episode len, obs_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run policy: we run a fixed number of episodes and we collect the relative trajectories\n",
    "def run_policy(env, policy, scaler, logger, episodes):\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, unscaled_observes = run_episode(env, policy, scaler)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes, 'actions': actions, 'rewards': rewards,\n",
    "                      'unscaled_observes': unscaled_observes}\n",
    "        trajectories.append(trajectory)\n",
    "    unscaled = np.concatenate([t['unscaled_observes'] for t in trajectories])\n",
    "    scaler.update(unscaled)  # update running statistics for scaling observations\n",
    "    if logger is not None:\n",
    "        logger.log({'_MeanReward': np.mean([t['rewards'].sum() for t in trajectories]), 'Steps': total_steps})\n",
    "\n",
    "    return trajectories # Returns: a list of trajectory dictionaries (observes, actions, rewards, unscaled_observes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds estimated value to all time steps of all trajectories\n",
    "def add_value(trajectories, value_func): # value_func: takes observations and returns predicted state value\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = value_func.predict(observes)\n",
    "        trajectory['values'] = values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute discounted forward sum of a sequence at each point\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
    "\n",
    "# Compute thte discount sum rewards and the advantages of all trajectories\n",
    "def add_discount_sum_reward_and_advantage(trajectories, gamma):\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        disc_sum_rew = discount(rewards, gamma)\n",
    "        trajectory['disc_sum_rew'] = disc_sum_rew\n",
    "        values = trajectory['values']\n",
    "        \n",
    "        # temporal difference error to estimate advantage\n",
    "        advantages = rewards - values + np.append(values[1:] * gamma, 0)\n",
    "        trajectory['advantages'] = advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log various batch statistics\n",
    "def log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode):\n",
    "    logger.log({'_mean_obs': np.mean(observes), '_min_obs': np.min(observes),\n",
    "                '_max_obs': np.max(observes), '_std_obs': np.mean(np.var(observes, axis=0)),\n",
    "                '_mean_act': np.mean(actions), '_min_act': np.min(actions),\n",
    "                '_max_act': np.max(actions), '_std_act': np.mean(np.var(actions, axis=0)),\n",
    "                '_mean_adv': np.mean(advantages), '_min_adv': np.min(advantages),\n",
    "                '_max_adv': np.max(advantages), '_std_adv': np.var(advantages),\n",
    "                '_mean_discrew': np.mean(disc_sum_rew), '_min_discrew': np.min(disc_sum_rew),\n",
    "                '_max_discrew': np.max(disc_sum_rew), '_std_discrew': np.var(disc_sum_rew), '_Episode': episode})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Params -- h1: 180, h2: 30, h3: 5, lr: 0.00183\n",
      "Policy Params -- h1: 180, h2: 103, h3: 60, lr: 8.87e-05, logvar_speed: 12\n",
      "***** Episode 20, Mean R = -305.9 *****\n",
      "Beta: 0.667\n",
      "ExplainedVarNew: -0.239\n",
      "ExplainedVarOld: -0.72\n",
      "KL: 0.000906\n",
      "PolicyEntropy: 5.49\n",
      "PolicyLoss: 0.00046\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0283\n",
      "\n",
      "\n",
      "***** Episode 40, Mean R = -296.1 *****\n",
      "Beta: 0.444\n",
      "ExplainedVarNew: -0.313\n",
      "ExplainedVarOld: -0.328\n",
      "KL: 0.000329\n",
      "PolicyEntropy: 5.49\n",
      "PolicyLoss: -0.000354\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0294\n",
      "\n",
      "\n",
      "***** Episode 60, Mean R = -290.6 *****\n",
      "Beta: 0.296\n",
      "ExplainedVarNew: -0.286\n",
      "ExplainedVarOld: -0.268\n",
      "KL: 0.000367\n",
      "PolicyEntropy: 5.49\n",
      "PolicyLoss: -0.000149\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0327\n",
      "\n",
      "\n",
      "***** Episode 80, Mean R = -322.9 *****\n",
      "Beta: 0.198\n",
      "ExplainedVarNew: -0.313\n",
      "ExplainedVarOld: -0.233\n",
      "KL: 0.000523\n",
      "PolicyEntropy: 5.48\n",
      "PolicyLoss: 7.03e-05\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0419\n",
      "\n",
      "\n",
      "***** Episode 100, Mean R = -283.4 *****\n",
      "Beta: 0.132\n",
      "ExplainedVarNew: -0.288\n",
      "ExplainedVarOld: -0.395\n",
      "KL: 0.000979\n",
      "PolicyEntropy: 5.48\n",
      "PolicyLoss: -0.000459\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0252\n",
      "\n",
      "\n",
      "***** Episode 120, Mean R = -249.2 *****\n",
      "Beta: 0.0878\n",
      "ExplainedVarNew: -0.33\n",
      "ExplainedVarOld: -0.298\n",
      "KL: 0.00156\n",
      "PolicyEntropy: 5.47\n",
      "PolicyLoss: -0.000658\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0301\n",
      "\n",
      "\n",
      "***** Episode 140, Mean R = -247.3 *****\n",
      "Beta: 0.0585\n",
      "ExplainedVarNew: -0.245\n",
      "ExplainedVarOld: -0.331\n",
      "KL: 0.00305\n",
      "PolicyEntropy: 5.45\n",
      "PolicyLoss: -0.000485\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0261\n",
      "\n",
      "\n",
      "***** Episode 160, Mean R = -203.8 *****\n",
      "Beta: 0.0585\n",
      "ExplainedVarNew: -0.287\n",
      "ExplainedVarOld: -0.299\n",
      "KL: 0.00593\n",
      "PolicyEntropy: 5.43\n",
      "PolicyLoss: -0.000811\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.021\n",
      "\n",
      "\n",
      "***** Episode 180, Mean R = -198.9 *****\n",
      "Beta: 0.0585\n",
      "ExplainedVarNew: -0.309\n",
      "ExplainedVarOld: -0.313\n",
      "KL: 0.00614\n",
      "PolicyEntropy: 5.41\n",
      "PolicyLoss: -0.000743\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0195\n",
      "\n",
      "\n",
      "***** Episode 200, Mean R = -179.9 *****\n",
      "Beta: 0.039\n",
      "ExplainedVarNew: -0.29\n",
      "ExplainedVarOld: -0.269\n",
      "KL: 0.00469\n",
      "PolicyEntropy: 5.38\n",
      "PolicyLoss: -0.000532\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0223\n",
      "\n",
      "\n",
      "***** Episode 220, Mean R = -167.9 *****\n",
      "Beta: 0.039\n",
      "ExplainedVarNew: -0.25\n",
      "ExplainedVarOld: -0.303\n",
      "KL: 0.00618\n",
      "PolicyEntropy: 5.36\n",
      "PolicyLoss: -0.000405\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0211\n",
      "\n",
      "\n",
      "***** Episode 240, Mean R = -133.7 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.287\n",
      "ExplainedVarOld: -0.27\n",
      "KL: 0.00373\n",
      "PolicyEntropy: 5.32\n",
      "PolicyLoss: -0.000767\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0182\n",
      "\n",
      "\n",
      "***** Episode 260, Mean R = -144.2 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.303\n",
      "ExplainedVarOld: -0.322\n",
      "KL: 0.0137\n",
      "PolicyEntropy: 5.27\n",
      "PolicyLoss: -0.00082\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0173\n",
      "\n",
      "\n",
      "***** Episode 280, Mean R = -106.7 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.351\n",
      "ExplainedVarOld: -0.376\n",
      "KL: 0.0116\n",
      "PolicyEntropy: 5.23\n",
      "PolicyLoss: -0.00075\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0118\n",
      "\n",
      "\n",
      "***** Episode 300, Mean R = -94.9 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.295\n",
      "ExplainedVarOld: -0.328\n",
      "KL: 0.0137\n",
      "PolicyEntropy: 5.18\n",
      "PolicyLoss: -0.000803\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0143\n",
      "\n",
      "\n",
      "***** Episode 320, Mean R = -60.8 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.304\n",
      "ExplainedVarOld: -0.282\n",
      "KL: 0.00982\n",
      "PolicyEntropy: 5.12\n",
      "PolicyLoss: -0.000677\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0113\n",
      "\n",
      "\n",
      "***** Episode 340, Mean R = -41.0 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.235\n",
      "ExplainedVarOld: -0.278\n",
      "KL: 0.0273\n",
      "PolicyEntropy: 5.04\n",
      "PolicyLoss: -0.000916\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.00753\n",
      "\n",
      "\n",
      "***** Episode 360, Mean R = -44.3 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.219\n",
      "ExplainedVarOld: -0.155\n",
      "KL: 0.00416\n",
      "PolicyEntropy: 5\n",
      "PolicyLoss: -0.000324\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0111\n",
      "\n",
      "\n",
      "***** Episode 380, Mean R = -23.5 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.277\n",
      "ExplainedVarOld: -0.294\n",
      "KL: 0.0135\n",
      "PolicyEntropy: 4.98\n",
      "PolicyLoss: -0.000726\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.00767\n",
      "\n",
      "\n",
      "***** Episode 400, Mean R = 5.3 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.229\n",
      "ExplainedVarOld: -0.256\n",
      "KL: 0.00863\n",
      "PolicyEntropy: 4.91\n",
      "PolicyLoss: -0.000738\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.00852\n",
      "\n",
      "\n",
      "***** Episode 420, Mean R = 38.6 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.206\n",
      "ExplainedVarOld: -0.294\n",
      "KL: 0.0137\n",
      "PolicyEntropy: 4.84\n",
      "PolicyLoss: -0.00094\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.00606\n",
      "\n",
      "\n",
      "***** Episode 440, Mean R = 78.0 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.172\n",
      "ExplainedVarOld: -0.147\n",
      "KL: 0.0155\n",
      "PolicyEntropy: 4.78\n",
      "PolicyLoss: -0.00106\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.00879\n",
      "\n",
      "\n",
      "***** Episode 460, Mean R = 91.7 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.671\n",
      "ExplainedVarOld: -0.34\n",
      "KL: 0.0408\n",
      "PolicyEntropy: 4.75\n",
      "PolicyLoss: -0.0019\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0289\n",
      "\n",
      "\n",
      "***** Episode 480, Mean R = 176.7 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.275\n",
      "ExplainedVarOld: -0.227\n",
      "KL: 0.0154\n",
      "PolicyEntropy: 4.71\n",
      "PolicyLoss: -0.00147\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0111\n",
      "\n",
      "\n",
      "***** Episode 500, Mean R = 204.5 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.313\n",
      "ExplainedVarOld: -0.212\n",
      "KL: 0.0148\n",
      "PolicyEntropy: 4.66\n",
      "PolicyLoss: -0.00124\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0193\n",
      "\n",
      "\n",
      "***** Episode 520, Mean R = 251.1 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.433\n",
      "ExplainedVarOld: -0.372\n",
      "KL: 0.0137\n",
      "PolicyEntropy: 4.63\n",
      "PolicyLoss: -0.00135\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0296\n",
      "\n",
      "\n",
      "***** Episode 540, Mean R = 223.2 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.715\n",
      "ExplainedVarOld: -0.52\n",
      "KL: 0.0161\n",
      "PolicyEntropy: 4.61\n",
      "PolicyLoss: -0.00128\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0805\n",
      "\n",
      "\n",
      "***** Episode 560, Mean R = 237.6 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.697\n",
      "ExplainedVarOld: -0.652\n",
      "KL: 0.0136\n",
      "PolicyEntropy: 4.54\n",
      "PolicyLoss: -0.00128\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.113\n",
      "\n",
      "\n",
      "***** Episode 580, Mean R = 320.1 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.678\n",
      "ExplainedVarOld: -0.564\n",
      "KL: 0.0183\n",
      "PolicyEntropy: 4.46\n",
      "PolicyLoss: -0.0016\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0807\n",
      "\n",
      "\n",
      "***** Episode 600, Mean R = 303.3 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.735\n",
      "ExplainedVarOld: -0.696\n",
      "KL: 0.0184\n",
      "PolicyEntropy: 4.39\n",
      "PolicyLoss: -0.00136\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.126\n",
      "\n",
      "\n",
      "***** Episode 620, Mean R = 457.5 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.518\n",
      "ExplainedVarOld: -0.492\n",
      "KL: 0.0199\n",
      "PolicyEntropy: 4.31\n",
      "PolicyLoss: -0.00176\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0292\n",
      "\n",
      "\n",
      "***** Episode 640, Mean R = 512.5 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.369\n",
      "ExplainedVarOld: -0.353\n",
      "KL: 0.0265\n",
      "PolicyEntropy: 4.23\n",
      "PolicyLoss: -0.00198\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0405\n",
      "\n",
      "\n",
      "***** Episode 660, Mean R = 463.4 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.771\n",
      "ExplainedVarOld: -0.794\n",
      "KL: 0.0102\n",
      "PolicyEntropy: 4.16\n",
      "PolicyLoss: -0.0017\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.184\n",
      "\n",
      "\n",
      "***** Episode 680, Mean R = 615.9 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.598\n",
      "ExplainedVarOld: -0.506\n",
      "KL: 0.0123\n",
      "PolicyEntropy: 4.08\n",
      "PolicyLoss: -0.0019\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0888\n",
      "\n",
      "\n",
      "***** Episode 700, Mean R = 697.0 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.474\n",
      "ExplainedVarOld: -0.446\n",
      "KL: 0.0135\n",
      "PolicyEntropy: 4\n",
      "PolicyLoss: -0.00199\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0687\n",
      "\n",
      "\n",
      "***** Episode 720, Mean R = 750.0 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.704\n",
      "ExplainedVarOld: -0.554\n",
      "KL: 0.0118\n",
      "PolicyEntropy: 3.98\n",
      "PolicyLoss: -0.00199\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.122\n",
      "\n",
      "\n",
      "***** Episode 740, Mean R = 818.6 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.53\n",
      "ExplainedVarOld: -0.559\n",
      "KL: 0.0144\n",
      "PolicyEntropy: 3.93\n",
      "PolicyLoss: -0.00191\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.0805\n",
      "\n",
      "\n",
      "***** Episode 760, Mean R = 762.0 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.849\n",
      "ExplainedVarOld: -0.761\n",
      "KL: 0.0117\n",
      "PolicyEntropy: 3.89\n",
      "PolicyLoss: -0.00148\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.262\n",
      "\n",
      "\n",
      "***** Episode 780, Mean R = 672.9 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.911\n",
      "ExplainedVarOld: -0.869\n",
      "KL: 0.0101\n",
      "PolicyEntropy: 3.83\n",
      "PolicyLoss: -0.00136\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.462\n",
      "\n",
      "\n",
      "***** Episode 800, Mean R = 893.5 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.629\n",
      "ExplainedVarOld: -0.656\n",
      "KL: 0.016\n",
      "PolicyEntropy: 3.75\n",
      "PolicyLoss: -0.00177\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.177\n",
      "\n",
      "\n",
      "***** Episode 820, Mean R = 971.2 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.737\n",
      "ExplainedVarOld: -0.605\n",
      "KL: 0.0123\n",
      "PolicyEntropy: 3.7\n",
      "PolicyLoss: -0.00254\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.286\n",
      "\n",
      "\n",
      "***** Episode 840, Mean R = 807.2 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.903\n",
      "ExplainedVarOld: -0.825\n",
      "KL: 0.0164\n",
      "PolicyEntropy: 3.67\n",
      "PolicyLoss: -0.00178\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.524\n",
      "\n",
      "\n",
      "***** Episode 860, Mean R = 949.7 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.853\n",
      "ExplainedVarOld: -0.795\n",
      "KL: 0.0149\n",
      "PolicyEntropy: 3.6\n",
      "PolicyLoss: -0.002\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.491\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Episode 880, Mean R = 1139.5 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.742\n",
      "ExplainedVarOld: -0.682\n",
      "KL: 0.0273\n",
      "PolicyEntropy: 3.52\n",
      "PolicyLoss: -0.00315\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.271\n",
      "\n",
      "\n",
      "***** Episode 900, Mean R = 1224.7 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.742\n",
      "ExplainedVarOld: -0.61\n",
      "KL: 0.0094\n",
      "PolicyEntropy: 3.46\n",
      "PolicyLoss: -0.002\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.193\n",
      "\n",
      "\n",
      "***** Episode 920, Mean R = 1249.6 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.753\n",
      "ExplainedVarOld: -0.749\n",
      "KL: 0.00847\n",
      "PolicyEntropy: 3.42\n",
      "PolicyLoss: -0.00197\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.312\n",
      "\n",
      "\n",
      "***** Episode 940, Mean R = 1334.1 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.755\n",
      "ExplainedVarOld: -0.67\n",
      "KL: 0.00891\n",
      "PolicyEntropy: 3.33\n",
      "PolicyLoss: -0.00181\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.213\n",
      "\n",
      "\n",
      "***** Episode 960, Mean R = 1426.4 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.716\n",
      "ExplainedVarOld: -0.781\n",
      "KL: 0.00765\n",
      "PolicyEntropy: 3.27\n",
      "PolicyLoss: -0.00222\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.193\n",
      "\n",
      "\n",
      "***** Episode 980, Mean R = 1453.0 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.693\n",
      "ExplainedVarOld: -0.651\n",
      "KL: 0.00898\n",
      "PolicyEntropy: 3.23\n",
      "PolicyLoss: -0.00166\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.215\n",
      "\n",
      "\n",
      "***** Episode 1000, Mean R = 1283.3 *****\n",
      "Beta: 0.145\n",
      "ExplainedVarNew: -0.873\n",
      "ExplainedVarOld: -1.36\n",
      "KL: 0.0416\n",
      "PolicyEntropy: 3.21\n",
      "PolicyLoss: -0.00362\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.739\n",
      "\n",
      "\n",
      "***** Episode 1020, Mean R = 1405.1 *****\n",
      "Beta: 0.145\n",
      "ExplainedVarNew: -0.803\n",
      "ExplainedVarOld: -0.791\n",
      "KL: 0.00519\n",
      "PolicyEntropy: 3.2\n",
      "PolicyLoss: -0.00116\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.408\n",
      "\n",
      "\n",
      "***** Episode 1040, Mean R = 1413.0 *****\n",
      "Beta: 0.145\n",
      "ExplainedVarNew: -0.817\n",
      "ExplainedVarOld: -0.765\n",
      "KL: 0.00502\n",
      "PolicyEntropy: 3.16\n",
      "PolicyLoss: -0.00151\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.59\n",
      "\n",
      "\n",
      "***** Episode 1060, Mean R = 1556.2 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.696\n",
      "ExplainedVarOld: -0.733\n",
      "KL: 0.00384\n",
      "PolicyEntropy: 3.12\n",
      "PolicyLoss: -0.00157\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.242\n",
      "\n",
      "\n",
      "***** Episode 1080, Mean R = 1496.1 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.859\n",
      "ExplainedVarOld: -0.787\n",
      "KL: 0.0092\n",
      "PolicyEntropy: 3.06\n",
      "PolicyLoss: -0.00161\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.448\n",
      "\n",
      "\n",
      "***** Episode 1100, Mean R = 1596.9 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.761\n",
      "ExplainedVarOld: -0.749\n",
      "KL: 0.00751\n",
      "PolicyEntropy: 3.02\n",
      "PolicyLoss: -0.00156\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.347\n",
      "\n",
      "\n",
      "***** Episode 1120, Mean R = 1576.3 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.826\n",
      "ExplainedVarOld: -0.787\n",
      "KL: 0.00646\n",
      "PolicyEntropy: 2.95\n",
      "PolicyLoss: -0.00129\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.448\n",
      "\n",
      "\n",
      "***** Episode 1140, Mean R = 1620.5 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.812\n",
      "ExplainedVarOld: -0.762\n",
      "KL: 0.00783\n",
      "PolicyEntropy: 2.91\n",
      "PolicyLoss: -0.00183\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.572\n",
      "\n",
      "\n",
      "***** Episode 1160, Mean R = 1573.6 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.884\n",
      "ExplainedVarOld: -0.876\n",
      "KL: 0.0075\n",
      "PolicyEntropy: 2.86\n",
      "PolicyLoss: -0.00122\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.626\n",
      "\n",
      "\n",
      "***** Episode 1180, Mean R = 1716.5 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.759\n",
      "ExplainedVarOld: -0.746\n",
      "KL: 0.00915\n",
      "PolicyEntropy: 2.79\n",
      "PolicyLoss: -0.00178\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.363\n",
      "\n",
      "\n",
      "***** Episode 1200, Mean R = 1674.3 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.858\n",
      "ExplainedVarOld: -0.846\n",
      "KL: 0.0075\n",
      "PolicyEntropy: 2.73\n",
      "PolicyLoss: -0.00161\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.588\n",
      "\n",
      "\n",
      "***** Episode 1220, Mean R = 1683.8 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.819\n",
      "ExplainedVarOld: -0.783\n",
      "KL: 0.00761\n",
      "PolicyEntropy: 2.69\n",
      "PolicyLoss: -0.00168\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.852\n",
      "\n",
      "\n",
      "***** Episode 1240, Mean R = 1586.2 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.893\n",
      "ExplainedVarOld: -0.856\n",
      "KL: 0.00854\n",
      "PolicyEntropy: 2.64\n",
      "PolicyLoss: -0.00149\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.17\n",
      "\n",
      "\n",
      "***** Episode 1260, Mean R = 1820.5 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.813\n",
      "ExplainedVarOld: -0.78\n",
      "KL: 0.00914\n",
      "PolicyEntropy: 2.59\n",
      "PolicyLoss: -0.00197\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.725\n",
      "\n",
      "\n",
      "***** Episode 1280, Mean R = 1956.9 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.702\n",
      "ExplainedVarOld: -0.738\n",
      "KL: 0.0136\n",
      "PolicyEntropy: 2.56\n",
      "PolicyLoss: -0.00282\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.355\n",
      "\n",
      "\n",
      "***** Episode 1300, Mean R = 1943.9 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.863\n",
      "ExplainedVarOld: -0.74\n",
      "KL: 0.00868\n",
      "PolicyEntropy: 2.49\n",
      "PolicyLoss: -0.00116\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.361\n",
      "\n",
      "\n",
      "***** Episode 1320, Mean R = 2056.7 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.792\n",
      "ExplainedVarOld: -0.794\n",
      "KL: 0.00795\n",
      "PolicyEntropy: 2.42\n",
      "PolicyLoss: -0.00182\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.37\n",
      "\n",
      "\n",
      "***** Episode 1340, Mean R = 2054.6 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.868\n",
      "ExplainedVarOld: -0.773\n",
      "KL: 0.00964\n",
      "PolicyEntropy: 2.36\n",
      "PolicyLoss: -0.00173\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.531\n",
      "\n",
      "\n",
      "***** Episode 1360, Mean R = 1896.1 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.906\n",
      "ExplainedVarOld: -0.845\n",
      "KL: 0.00853\n",
      "PolicyEntropy: 2.3\n",
      "PolicyLoss: -0.000893\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.04\n",
      "\n",
      "\n",
      "***** Episode 1380, Mean R = 2156.3 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.771\n",
      "ExplainedVarOld: -0.707\n",
      "KL: 0.00884\n",
      "PolicyEntropy: 2.24\n",
      "PolicyLoss: -0.00196\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.506\n",
      "\n",
      "\n",
      "***** Episode 1400, Mean R = 2049.5 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.948\n",
      "ExplainedVarOld: -0.898\n",
      "KL: 0.0127\n",
      "PolicyEntropy: 2.16\n",
      "PolicyLoss: -0.0022\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.906\n",
      "\n",
      "\n",
      "***** Episode 1420, Mean R = 2221.7 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.782\n",
      "ExplainedVarOld: -0.916\n",
      "KL: 0.00798\n",
      "PolicyEntropy: 2.12\n",
      "PolicyLoss: -0.00193\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.443\n",
      "\n",
      "\n",
      "***** Episode 1440, Mean R = 2258.5 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.834\n",
      "ExplainedVarOld: -0.754\n",
      "KL: 0.00835\n",
      "PolicyEntropy: 2.08\n",
      "PolicyLoss: -0.00171\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.468\n",
      "\n",
      "\n",
      "***** Episode 1460, Mean R = 2355.2 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.825\n",
      "ExplainedVarOld: -0.779\n",
      "KL: 0.00703\n",
      "PolicyEntropy: 2.03\n",
      "PolicyLoss: -0.00196\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.576\n",
      "\n",
      "\n",
      "***** Episode 1480, Mean R = 2392.6 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.89\n",
      "ExplainedVarOld: -0.784\n",
      "KL: 0.00741\n",
      "PolicyEntropy: 1.96\n",
      "PolicyLoss: -0.00203\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.632\n",
      "\n",
      "\n",
      "***** Episode 1500, Mean R = 2417.5 *****\n",
      "Beta: 0.145\n",
      "ExplainedVarNew: -0.859\n",
      "ExplainedVarOld: -0.895\n",
      "KL: 0.0339\n",
      "PolicyEntropy: 1.89\n",
      "PolicyLoss: -0.00626\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.582\n",
      "\n",
      "\n",
      "***** Episode 1520, Mean R = 2419.7 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.833\n",
      "ExplainedVarOld: -0.821\n",
      "KL: 0.00488\n",
      "PolicyEntropy: 1.85\n",
      "PolicyLoss: -0.000475\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.58\n",
      "\n",
      "\n",
      "***** Episode 1540, Mean R = 2469.3 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.885\n",
      "ExplainedVarOld: -0.795\n",
      "KL: 0.00647\n",
      "PolicyEntropy: 1.79\n",
      "PolicyLoss: -0.00139\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.628\n",
      "\n",
      "\n",
      "***** Episode 1560, Mean R = 2546.3 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.962\n",
      "ExplainedVarOld: -0.918\n",
      "KL: 0.00676\n",
      "PolicyEntropy: 1.73\n",
      "PolicyLoss: -0.0015\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.616\n",
      "\n",
      "\n",
      "***** Episode 1580, Mean R = 2553.5 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.875\n",
      "ExplainedVarOld: -0.916\n",
      "KL: 0.00741\n",
      "PolicyEntropy: 1.65\n",
      "PolicyLoss: -0.00148\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.629\n",
      "\n",
      "\n",
      "***** Episode 1600, Mean R = 2653.4 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.972\n",
      "ExplainedVarOld: -0.843\n",
      "KL: 0.00596\n",
      "PolicyEntropy: 1.58\n",
      "PolicyLoss: -0.0017\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.674\n",
      "\n",
      "\n",
      "***** Episode 1620, Mean R = 2661.7 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.949\n",
      "ExplainedVarOld: -1\n",
      "KL: 0.0077\n",
      "PolicyEntropy: 1.51\n",
      "PolicyLoss: -0.00145\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.634\n",
      "\n",
      "\n",
      "***** Episode 1640, Mean R = 2749.4 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.925\n",
      "ExplainedVarOld: -0.899\n",
      "KL: 0.00478\n",
      "PolicyEntropy: 1.44\n",
      "PolicyLoss: -0.00153\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.666\n",
      "\n",
      "\n",
      "***** Episode 1660, Mean R = 2791.8 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.956\n",
      "ExplainedVarOld: -0.91\n",
      "KL: 0.00946\n",
      "PolicyEntropy: 1.35\n",
      "PolicyLoss: -0.00118\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.69\n",
      "\n",
      "\n",
      "***** Episode 1680, Mean R = 2785.9 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.881\n",
      "ExplainedVarOld: -0.912\n",
      "KL: 0.0092\n",
      "PolicyEntropy: 1.29\n",
      "PolicyLoss: -0.0011\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.698\n",
      "\n",
      "\n",
      "***** Episode 1700, Mean R = 2895.3 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.929\n",
      "ExplainedVarOld: -0.842\n",
      "KL: 0.00688\n",
      "PolicyEntropy: 1.22\n",
      "PolicyLoss: -0.00144\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.747\n",
      "\n",
      "\n",
      "***** Episode 1720, Mean R = 2933.7 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.921\n",
      "ExplainedVarOld: -0.896\n",
      "KL: 0.0121\n",
      "PolicyEntropy: 1.14\n",
      "PolicyLoss: -0.00225\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.788\n",
      "\n",
      "\n",
      "***** Episode 1740, Mean R = 3007.4 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.904\n",
      "ExplainedVarOld: -0.924\n",
      "KL: 0.00816\n",
      "PolicyEntropy: 1.06\n",
      "PolicyLoss: -0.00163\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.755\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Episode 1760, Mean R = 3025.2 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.923\n",
      "ExplainedVarOld: -0.852\n",
      "KL: 0.00942\n",
      "PolicyEntropy: 0.976\n",
      "PolicyLoss: -0.00128\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.83\n",
      "\n",
      "\n",
      "***** Episode 1780, Mean R = 3072.0 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.964\n",
      "ExplainedVarOld: -0.923\n",
      "KL: 0.00933\n",
      "PolicyEntropy: 0.893\n",
      "PolicyLoss: -0.00128\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.836\n",
      "\n",
      "\n",
      "***** Episode 1800, Mean R = 3093.5 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.967\n",
      "ExplainedVarOld: -0.977\n",
      "KL: 0.00886\n",
      "PolicyEntropy: 0.819\n",
      "PolicyLoss: -0.00092\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.834\n",
      "\n",
      "\n",
      "***** Episode 1820, Mean R = 3152.3 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.942\n",
      "ExplainedVarOld: -0.934\n",
      "KL: 0.00684\n",
      "PolicyEntropy: 0.739\n",
      "PolicyLoss: -0.00111\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.848\n",
      "\n",
      "\n",
      "***** Episode 1840, Mean R = 3206.0 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.917\n",
      "ExplainedVarOld: -0.896\n",
      "KL: 0.00705\n",
      "PolicyEntropy: 0.679\n",
      "PolicyLoss: -0.00122\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.881\n",
      "\n",
      "\n",
      "***** Episode 1860, Mean R = 3228.1 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.999\n",
      "ExplainedVarOld: -0.928\n",
      "KL: 0.00864\n",
      "PolicyEntropy: 0.591\n",
      "PolicyLoss: -0.00116\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.887\n",
      "\n",
      "\n",
      "***** Episode 1880, Mean R = 3248.4 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.991\n",
      "ExplainedVarOld: -0.968\n",
      "KL: 0.00823\n",
      "PolicyEntropy: 0.523\n",
      "PolicyLoss: -0.000982\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.921\n",
      "\n",
      "\n",
      "***** Episode 1900, Mean R = 3295.4 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.984\n",
      "ExplainedVarOld: -0.957\n",
      "KL: 0.00832\n",
      "PolicyEntropy: 0.446\n",
      "PolicyLoss: -0.00128\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.947\n",
      "\n",
      "\n",
      "***** Episode 1920, Mean R = 3300.6 *****\n",
      "Beta: 0.0964\n",
      "ExplainedVarNew: -0.942\n",
      "ExplainedVarOld: -1.08\n",
      "KL: 0.0431\n",
      "PolicyEntropy: 0.336\n",
      "PolicyLoss: -0.00542\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.981\n",
      "\n",
      "\n",
      "***** Episode 1940, Mean R = 3370.2 *****\n",
      "Beta: 0.145\n",
      "ExplainedVarNew: -0.956\n",
      "ExplainedVarOld: -0.944\n",
      "KL: 0.06\n",
      "PolicyEntropy: 0.313\n",
      "PolicyLoss: 0.00161\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.956\n",
      "\n",
      "\n",
      "***** Episode 1960, Mean R = 3370.3 *****\n",
      "Beta: 0.217\n",
      "ExplainedVarNew: -0.996\n",
      "ExplainedVarOld: -0.963\n",
      "KL: 0.104\n",
      "PolicyEntropy: 0.307\n",
      "PolicyLoss: -0.000179\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.983\n",
      "\n",
      "\n",
      "***** Episode 1980, Mean R = 3286.5 *****\n",
      "Beta: 0.325\n",
      "ExplainedVarNew: -1.01\n",
      "ExplainedVarOld: -1.04\n",
      "KL: 0.0544\n",
      "PolicyEntropy: 0.301\n",
      "PolicyLoss: 0.000642\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.951\n",
      "\n",
      "\n",
      "***** Episode 2000, Mean R = 3174.4 *****\n",
      "Beta: 0.488\n",
      "ExplainedVarNew: -0.977\n",
      "ExplainedVarOld: -1.09\n",
      "KL: 0.439\n",
      "PolicyEntropy: 0.289\n",
      "PolicyLoss: 0.00811\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.866\n",
      "\n",
      "\n",
      "***** Episode 2020, Mean R = 3394.4 *****\n",
      "Beta: 0.732\n",
      "ExplainedVarNew: -0.895\n",
      "ExplainedVarOld: -0.837\n",
      "KL: 0.585\n",
      "PolicyEntropy: 0.284\n",
      "PolicyLoss: -0.00113\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.987\n",
      "\n",
      "\n",
      "***** Episode 2040, Mean R = 3195.0 *****\n",
      "Beta: 1.1\n",
      "ExplainedVarNew: -0.973\n",
      "ExplainedVarOld: -0.732\n",
      "KL: 0.57\n",
      "PolicyEntropy: 0.278\n",
      "PolicyLoss: 0.00114\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.37\n",
      "\n",
      "\n",
      "***** Episode 2060, Mean R = 2607.3 *****\n",
      "Beta: 1.65\n",
      "ExplainedVarNew: -1.08\n",
      "ExplainedVarOld: -1.13\n",
      "KL: 0.283\n",
      "PolicyEntropy: 0.272\n",
      "PolicyLoss: 0.00556\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.19\n",
      "\n",
      "\n",
      "***** Episode 2080, Mean R = 1843.2 *****\n",
      "Beta: 2.47\n",
      "ExplainedVarNew: -1.11\n",
      "ExplainedVarOld: -1.39\n",
      "KL: 0.078\n",
      "PolicyEntropy: 0.266\n",
      "PolicyLoss: 0.00623\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.08\n",
      "\n",
      "\n",
      "***** Episode 2100, Mean R = 1681.6 *****\n",
      "Beta: 3.71\n",
      "ExplainedVarNew: -0.99\n",
      "ExplainedVarOld: -1.28\n",
      "KL: 0.2\n",
      "PolicyEntropy: 0.267\n",
      "PolicyLoss: 0.0404\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.453\n",
      "\n",
      "\n",
      "***** Episode 2120, Mean R = 1667.2 *****\n",
      "Beta: 5.56\n",
      "ExplainedVarNew: -0.932\n",
      "ExplainedVarOld: -1.09\n",
      "KL: 0.254\n",
      "PolicyEntropy: 0.273\n",
      "PolicyLoss: 0.00104\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.371\n",
      "\n",
      "\n",
      "***** Episode 2140, Mean R = 1682.7 *****\n",
      "Beta: 8.34\n",
      "ExplainedVarNew: -0.937\n",
      "ExplainedVarOld: -1\n",
      "KL: 0.207\n",
      "PolicyEntropy: 0.277\n",
      "PolicyLoss: -0.00014\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.336\n",
      "\n",
      "\n",
      "***** Episode 2160, Mean R = 1646.9 *****\n",
      "Beta: 12.5\n",
      "ExplainedVarNew: -0.895\n",
      "ExplainedVarOld: -0.531\n",
      "KL: 0.168\n",
      "PolicyEntropy: 0.281\n",
      "PolicyLoss: 0.00034\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.621\n",
      "\n",
      "\n",
      "***** Episode 2180, Mean R = 1766.7 *****\n",
      "Beta: 18.8\n",
      "ExplainedVarNew: -0.947\n",
      "ExplainedVarOld: -0.914\n",
      "KL: 0.145\n",
      "PolicyEntropy: 0.283\n",
      "PolicyLoss: -0.000499\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.318\n",
      "\n",
      "\n",
      "***** Episode 2200, Mean R = 1708.0 *****\n",
      "Beta: 28.2\n",
      "ExplainedVarNew: -0.919\n",
      "ExplainedVarOld: -0.905\n",
      "KL: 0.118\n",
      "PolicyEntropy: 0.284\n",
      "PolicyLoss: -0.000127\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.661\n",
      "\n",
      "\n",
      "***** Episode 2220, Mean R = 1783.4 *****\n",
      "Beta: 35\n",
      "ExplainedVarNew: -0.836\n",
      "ExplainedVarOld: -0.88\n",
      "KL: 0.0994\n",
      "PolicyEntropy: 0.286\n",
      "PolicyLoss: -0.000142\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.353\n",
      "\n",
      "\n",
      "***** Episode 2240, Mean R = 1825.7 *****\n",
      "Beta: 35\n",
      "ExplainedVarNew: -0.964\n",
      "ExplainedVarOld: -0.875\n",
      "KL: 0.153\n",
      "PolicyEntropy: 0.299\n",
      "PolicyLoss: 1.32\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.336\n",
      "\n",
      "\n",
      "***** Episode 2260, Mean R = 1556.8 *****\n",
      "Beta: 35\n",
      "ExplainedVarNew: -1.03\n",
      "ExplainedVarOld: -0.878\n",
      "KL: 0.107\n",
      "PolicyEntropy: 0.306\n",
      "PolicyLoss: 0.00156\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.669\n",
      "\n",
      "\n",
      "***** Episode 2280, Mean R = 1527.4 *****\n",
      "Beta: 23.3\n",
      "ExplainedVarNew: -0.94\n",
      "ExplainedVarOld: -1.02\n",
      "KL: 0.00157\n",
      "PolicyEntropy: 0.31\n",
      "PolicyLoss: 0.0976\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.271\n",
      "\n",
      "\n",
      "***** Episode 2300, Mean R = 1545.4 *****\n",
      "Beta: 15.6\n",
      "ExplainedVarNew: -0.89\n",
      "ExplainedVarOld: -0.941\n",
      "KL: 0.000364\n",
      "PolicyEntropy: 0.309\n",
      "PolicyLoss: 0.00517\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.259\n",
      "\n",
      "\n",
      "***** Episode 2320, Mean R = 1528.7 *****\n",
      "Beta: 10.4\n",
      "ExplainedVarNew: -0.902\n",
      "ExplainedVarOld: -0.925\n",
      "KL: 3.16e-05\n",
      "PolicyEntropy: 0.31\n",
      "PolicyLoss: 0.00062\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.249\n",
      "\n",
      "\n",
      "***** Episode 2340, Mean R = 1521.6 *****\n",
      "Beta: 6.91\n",
      "ExplainedVarNew: -0.902\n",
      "ExplainedVarOld: -0.894\n",
      "KL: 1.31e-06\n",
      "PolicyEntropy: 0.309\n",
      "PolicyLoss: -0.000113\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.337\n",
      "\n",
      "\n",
      "***** Episode 2360, Mean R = 1496.8 *****\n",
      "Beta: 4.61\n",
      "ExplainedVarNew: -0.909\n",
      "ExplainedVarOld: -0.925\n",
      "KL: 5.71e-07\n",
      "PolicyEntropy: 0.309\n",
      "PolicyLoss: 0.000116\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.315\n",
      "\n",
      "\n",
      "***** Episode 2380, Mean R = 1528.0 *****\n",
      "Beta: 3.07\n",
      "ExplainedVarNew: -0.82\n",
      "ExplainedVarOld: -0.953\n",
      "KL: 1.12e-06\n",
      "PolicyEntropy: 0.309\n",
      "PolicyLoss: -0.00011\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.234\n",
      "\n",
      "\n",
      "***** Episode 2400, Mean R = 1484.0 *****\n",
      "Beta: 2.05\n",
      "ExplainedVarNew: -0.838\n",
      "ExplainedVarOld: -0.842\n",
      "KL: 3.2e-06\n",
      "PolicyEntropy: 0.309\n",
      "PolicyLoss: 0.000385\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.305\n",
      "\n",
      "\n",
      "***** Episode 2420, Mean R = 1464.0 *****\n",
      "Beta: 1.37\n",
      "ExplainedVarNew: -0.878\n",
      "ExplainedVarOld: -0.862\n",
      "KL: 4.98e-06\n",
      "PolicyEntropy: 0.308\n",
      "PolicyLoss: -0.000153\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.492\n",
      "\n",
      "\n",
      "***** Episode 2440, Mean R = 1515.2 *****\n",
      "Beta: 0.91\n",
      "ExplainedVarNew: -0.816\n",
      "ExplainedVarOld: -0.834\n",
      "KL: 8.97e-06\n",
      "PolicyEntropy: 0.307\n",
      "PolicyLoss: -9.85e-06\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.251\n",
      "\n",
      "\n",
      "***** Episode 2460, Mean R = 1530.3 *****\n",
      "Beta: 0.607\n",
      "ExplainedVarNew: -0.892\n",
      "ExplainedVarOld: -0.786\n",
      "KL: 1.47e-05\n",
      "PolicyEntropy: 0.306\n",
      "PolicyLoss: -5.28e-05\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.279\n",
      "\n",
      "\n",
      "***** Episode 2480, Mean R = 1541.6 *****\n",
      "Beta: 0.405\n",
      "ExplainedVarNew: -0.891\n",
      "ExplainedVarOld: -0.883\n",
      "KL: 2.32e-05\n",
      "PolicyEntropy: 0.305\n",
      "PolicyLoss: -0.000251\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.281\n",
      "\n",
      "\n",
      "***** Episode 2500, Mean R = 1534.7 *****\n",
      "Beta: 0.27\n",
      "ExplainedVarNew: -0.973\n",
      "ExplainedVarOld: -0.974\n",
      "KL: 3.38e-05\n",
      "PolicyEntropy: 0.303\n",
      "PolicyLoss: 0.00012\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.264\n",
      "\n",
      "\n",
      "***** Episode 2520, Mean R = 1524.6 *****\n",
      "Beta: 0.18\n",
      "ExplainedVarNew: -0.887\n",
      "ExplainedVarOld: -0.968\n",
      "KL: 6.15e-05\n",
      "PolicyEntropy: 0.302\n",
      "PolicyLoss: -0.000111\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.256\n",
      "\n",
      "\n",
      "***** Episode 2540, Mean R = 1523.6 *****\n",
      "Beta: 0.12\n",
      "ExplainedVarNew: -0.889\n",
      "ExplainedVarOld: -0.921\n",
      "KL: 5.98e-05\n",
      "PolicyEntropy: 0.3\n",
      "PolicyLoss: 1.56e-06\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.242\n",
      "\n",
      "\n",
      "***** Episode 2560, Mean R = 1463.4 *****\n",
      "Beta: 0.0799\n",
      "ExplainedVarNew: -0.912\n",
      "ExplainedVarOld: -0.482\n",
      "KL: 8.78e-05\n",
      "PolicyEntropy: 0.298\n",
      "PolicyLoss: 0.000452\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.441\n",
      "\n",
      "\n",
      "***** Episode 2580, Mean R = 1550.2 *****\n",
      "Beta: 0.0533\n",
      "ExplainedVarNew: -0.812\n",
      "ExplainedVarOld: -0.856\n",
      "KL: 8e-05\n",
      "PolicyEntropy: 0.296\n",
      "PolicyLoss: -0.000313\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.245\n",
      "\n",
      "\n",
      "***** Episode 2600, Mean R = 1525.4 *****\n",
      "Beta: 0.0355\n",
      "ExplainedVarNew: -0.877\n",
      "ExplainedVarOld: -0.864\n",
      "KL: 6.26e-05\n",
      "PolicyEntropy: 0.295\n",
      "PolicyLoss: -0.000323\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.476\n",
      "\n",
      "\n",
      "***** Episode 2620, Mean R = 1566.2 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.94\n",
      "ExplainedVarOld: -0.89\n",
      "KL: 9.5e-05\n",
      "PolicyEntropy: 0.295\n",
      "PolicyLoss: 2.16e-05\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.267\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Episode 2640, Mean R = 1568.6 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.809\n",
      "ExplainedVarOld: -0.897\n",
      "KL: 0.000241\n",
      "PolicyEntropy: 0.295\n",
      "PolicyLoss: -5.33e-05\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.263\n",
      "\n",
      "\n",
      "***** Episode 2660, Mean R = 1593.3 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.992\n",
      "ExplainedVarOld: -0.838\n",
      "KL: 0.000588\n",
      "PolicyEntropy: 0.293\n",
      "PolicyLoss: -0.00055\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.276\n",
      "\n",
      "\n",
      "***** Episode 2680, Mean R = 1581.9 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.924\n",
      "ExplainedVarOld: -0.953\n",
      "KL: 0.00149\n",
      "PolicyEntropy: 0.285\n",
      "PolicyLoss: -0.000247\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.282\n",
      "\n",
      "\n",
      "***** Episode 2700, Mean R = 1569.5 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.905\n",
      "ExplainedVarOld: -0.797\n",
      "KL: 0.00144\n",
      "PolicyEntropy: 0.278\n",
      "PolicyLoss: -0.000109\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.344\n",
      "\n",
      "\n",
      "***** Episode 2720, Mean R = 1633.9 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.887\n",
      "ExplainedVarOld: -0.835\n",
      "KL: 0.00229\n",
      "PolicyEntropy: 0.274\n",
      "PolicyLoss: -0.000585\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.294\n",
      "\n",
      "\n",
      "***** Episode 2740, Mean R = 1697.0 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.894\n",
      "ExplainedVarOld: -0.853\n",
      "KL: 0.00503\n",
      "PolicyEntropy: 0.273\n",
      "PolicyLoss: -0.00101\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.308\n",
      "\n",
      "\n",
      "***** Episode 2760, Mean R = 1715.3 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.877\n",
      "ExplainedVarOld: -0.838\n",
      "KL: 0.00882\n",
      "PolicyEntropy: 0.251\n",
      "PolicyLoss: -0.00143\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.502\n",
      "\n",
      "\n",
      "***** Episode 2780, Mean R = 1743.4 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.915\n",
      "ExplainedVarOld: -0.872\n",
      "KL: 0.0084\n",
      "PolicyEntropy: 0.259\n",
      "PolicyLoss: -0.00114\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.535\n",
      "\n",
      "\n",
      "***** Episode 2800, Mean R = 1767.2 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.876\n",
      "ExplainedVarOld: -0.837\n",
      "KL: 0.00812\n",
      "PolicyEntropy: 0.257\n",
      "PolicyLoss: -0.00127\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.673\n",
      "\n",
      "\n",
      "***** Episode 2820, Mean R = 1934.4 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.893\n",
      "ExplainedVarOld: -0.808\n",
      "KL: 0.0104\n",
      "PolicyEntropy: 0.27\n",
      "PolicyLoss: -0.00189\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.485\n",
      "\n",
      "\n",
      "***** Episode 2840, Mean R = 1979.9 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.848\n",
      "ExplainedVarOld: -0.853\n",
      "KL: 0.0119\n",
      "PolicyEntropy: 0.258\n",
      "PolicyLoss: -0.00138\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.414\n",
      "\n",
      "\n",
      "***** Episode 2860, Mean R = 1925.3 *****\n",
      "Beta: 0.0286\n",
      "ExplainedVarNew: -0.889\n",
      "ExplainedVarOld: -0.855\n",
      "KL: 0.0178\n",
      "PolicyEntropy: 0.259\n",
      "PolicyLoss: -0.00166\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.08\n",
      "\n",
      "\n",
      "***** Episode 2880, Mean R = 1837.9 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.882\n",
      "ExplainedVarOld: -0.865\n",
      "KL: 0.0209\n",
      "PolicyEntropy: 0.245\n",
      "PolicyLoss: -0.00153\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.75\n",
      "\n",
      "\n",
      "***** Episode 2900, Mean R = 1809.2 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.908\n",
      "ExplainedVarOld: -0.861\n",
      "KL: 0.0149\n",
      "PolicyEntropy: 0.224\n",
      "PolicyLoss: -0.00151\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 2.05\n",
      "\n",
      "\n",
      "***** Episode 2920, Mean R = 2110.9 *****\n",
      "Beta: 0.0429\n",
      "ExplainedVarNew: -0.889\n",
      "ExplainedVarOld: -0.821\n",
      "KL: 0.0115\n",
      "PolicyEntropy: 0.225\n",
      "PolicyLoss: -0.00119\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.12\n",
      "\n",
      "\n",
      "***** Episode 2940, Mean R = 2140.5 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.843\n",
      "ExplainedVarOld: -0.846\n",
      "KL: 0.0203\n",
      "PolicyEntropy: 0.234\n",
      "PolicyLoss: -0.00237\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.49\n",
      "\n",
      "\n",
      "***** Episode 2960, Mean R = 2228.4 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.88\n",
      "ExplainedVarOld: -0.817\n",
      "KL: 0.0143\n",
      "PolicyEntropy: 0.203\n",
      "PolicyLoss: -0.00209\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.52\n",
      "\n",
      "\n",
      "***** Episode 2980, Mean R = 2422.4 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.878\n",
      "ExplainedVarOld: -0.787\n",
      "KL: 0.0124\n",
      "PolicyEntropy: 0.176\n",
      "PolicyLoss: -0.00179\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 1.04\n",
      "\n",
      "\n",
      "***** Episode 3000, Mean R = 2507.1 *****\n",
      "Beta: 0.0643\n",
      "ExplainedVarNew: -0.928\n",
      "ExplainedVarOld: -0.877\n",
      "KL: 0.00891\n",
      "PolicyEntropy: 0.147\n",
      "PolicyLoss: -0.00127\n",
      "Steps: 2e+04\n",
      "ValFuncLoss: 0.99\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3000\n",
    "gamma = 0.995 \n",
    "batch_size = 20\n",
    "hid1_size = 10\n",
    "kl_targ = 0.01\n",
    "init_logvar = -1.0\n",
    "\n",
    "# Initialize gym environment\n",
    "env_name = \"HalfCheetah-v2\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim += 1  # add 1 to obs dimension for time step feature (see run_episode())\n",
    "\n",
    "# Initialize some global variables\n",
    "now = datetime.now().strftime(\"%b-%d_%H:%M:%S\")  # create unique directories for logs\n",
    "logger = Logger(logname=env_name, now=now) # Training log\n",
    "nameDirLogWeights = f\"log-weights/TRPO-HalfCheetah-v2-{now}\" # Weights log\n",
    "aigym_path = os.path.join('/tmp', env_name, now)\n",
    "env = wrappers.Monitor(env, aigym_path, force=True)\n",
    "scaler = Scaler(obs_dim)\n",
    "value_function = NNValueFunction(obs_dim, hid1_size)\n",
    "policy = Policy(obs_dim, act_dim, kl_targ, hid1_size, init_logvar)\n",
    "\n",
    "# run a few episodes of untrained policy to initialize scaler:\n",
    "run_policy(env, policy, scaler, logger, episodes=5)\n",
    "\n",
    "episode = 0\n",
    "while episode < num_episodes:\n",
    "    trajectories = run_policy(env, policy, scaler, logger, episodes=batch_size)\n",
    "    episode += len(trajectories)\n",
    "    add_value(trajectories, value_function)  # add estimated values to episodes\n",
    "    add_discount_sum_reward_and_advantage(trajectories, gamma) # calculate discounted sum of Rs and the advantage\n",
    "    # concatenate all episodes into single NumPy arrays\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    disc_sum_rew = np.concatenate([t['disc_sum_rew'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "    # add various stats to training log:\n",
    "    log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n",
    "    policy.update(observes, actions, advantages, logger)  # update policy\n",
    "    value_function.fit(observes, disc_sum_rew, logger, episode, env_name)  # update value function\n",
    "    logger.write(display=True)  # write logger results to file and stdout\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_episode(env, policy, scaler, animate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
