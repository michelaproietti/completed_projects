# Visual Question Answering

![VQA](https://visualqa.org/static/img/vqa_examples.jpg)

This project consists in the development of architectures that address the visual question answering (VQA) task, which is a semantic task that aims at answering 
questions based on an image.

In particular, starting from previous works, we have developed two trivial baselines (random and prior yes), a non trivial baseline, and a final more complex 
architecture, which can be considered state-of-the-art for the task, since it is the best-ranked model in PapersWithCode leaderboard.

It was implemented using PyTorch and other libraries, such as Scikit-learn, PIL and NLTK.

**Since this task is very complex and requires a huge amount of data, we do not focus on the obtained results but rather on the implementative challenges we encountered and on a comparison between the different implemented approaches.**

**Authors**:
* Federica Cocci
* Michela Proietti
* Sofia Santilli
