{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SIFID.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"bedkhSRih7rK","executionInfo":{"status":"ok","timestamp":1626962640593,"user_tz":-120,"elapsed":2389,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["import os, math, scipy, pickle, pathlib, sys\n","import numpy as np\n","from scipy import linalg\n","from matplotlib.pyplot import imread\n","from skimage.transform import resize\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D, UpSampling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.applications import inception_v3 as iv3\n","\n","try:\n","    from tqdm import tqdm\n","except ImportError:\n","    # If not tqdm is not available, provide a mock version of it\n","    def tqdm(x): return x"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueXyFnojcnWI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626962657826,"user_tz":-120,"elapsed":17236,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}},"outputId":"895257fb-e6a3-4d9a-84f9-c53530429f05"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yF0MCUFtlxwl","executionInfo":{"status":"ok","timestamp":1626962657833,"user_tz":-120,"elapsed":32,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["##### The following two blocks of code are needed to print the names of all the layers #####\n","##### of the Inception Network that are needed to get the SIFID score                  #####\n","\n","#inception = iv3.InceptionV3(input_shape=(299,299,3))\n","\n","#i = 0\n","#for layer in inception.layers:\n","#  sp = '                                 '[len(layer.name)-9:]\n","#  print(i, layer.name, sp, layer.trainable)\n","#  i += 1"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzRzZlgcpNig","executionInfo":{"status":"ok","timestamp":1626962657836,"user_tz":-120,"elapsed":34,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["#for i in range(11, 17, 3):\n","#  layer = inception.layers[i]\n","#  sp = '                                 '[len(layer.name)-9:]\n","#  print(layer.name, sp, layer.trainable)\n","\n","#print(inception.layers[10].name)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujiY_LDwiAvF","executionInfo":{"status":"ok","timestamp":1626962657841,"user_tz":-120,"elapsed":38,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["# Pretrained InceptionV3 network returning feature maps\n","class InceptionV3(keras.Model):\n","\n","    # Index of default block of inception to return, corresponds to output of final average pooling\n","    DEFAULT_BLOCK_INDEX = 3\n","\n","    # Maps feature dimensionality to their output blocks indices\n","    BLOCK_INDEX_BY_DIM = {\n","        64: 0,   # First max pooling features\n","        192: 1,  # Second max pooling features\n","        768: 2,  # Pre-aux classifier features\n","        2048: 3  # Final average pooling features\n","    }\n","\n","    def __init__(self,\n","                 output_blocks=[DEFAULT_BLOCK_INDEX],\n","                 resize_input=False,\n","                 normalize_input=True,\n","                 requires_grad=False):\n","        \"\"\"Build pretrained InceptionV3\n","        Parameters\n","        ----------\n","        output_blocks : list of int\n","            Indices of blocks to return features of. Possible values are:\n","                - 0: corresponds to output of first max pooling\n","                - 1: corresponds to output of second max pooling\n","                - 2: corresponds to output which is fed to aux classifier\n","                - 3: corresponds to output of final average pooling\n","        resize_input : bool\n","            If true, bilinearly resizes input to width and height 299 before\n","            feeding input to model. As the network without fully connected\n","            layers is fully convolutional, it should be able to handle inputs\n","            of arbitrary size, so resizing might not be strictly needed\n","        normalize_input : bool\n","            If true, scales the input from range (0, 1) to the range the\n","            pretrained Inception network expects, namely (-1, 1)\n","        requires_grad : bool\n","            If true, parameters of the model require gradient. Possibly useful\n","            for finetuning the network\n","        \"\"\"\n","        super(InceptionV3, self).__init__()\n","\n","        self.resize_input = resize_input\n","        self.normalize_input = normalize_input\n","        self.output_blocks = sorted(output_blocks)\n","        self.last_needed_block = max(output_blocks)\n","\n","        assert self.last_needed_block <= 3, \\\n","            'Last possible output block index is 3'\n","\n","        self.blocks = []\n","\n","        self.inception = iv3.InceptionV3(weights='imagenet')  # load pretrained InceptionV3 with weights trained on Imagenet\n","\n","        # Block 0: input to maxpool1 (3 conv2d layers)\n","        block0 = []\n","        for i in range(1, 10, 2):\n","          block0.append(self.inception.layers[i])\n","        self.blocks.append(Sequential(block0))\n","\n","        # Block 1: maxpool1 to maxpool2 (maxpool1 + 2 conv2d layers)\n","        if self.last_needed_block >= 1:\n","            block1 = [MaxPooling2D(pool_size=(3,3), strides=(2,2))]\n","            for i in range(11, 17, 3):\n","              block1.append(self.inception.layers[i])\n","            self.blocks.append(Sequential(block1))\n","\n","        # Block 2: maxpool2 to aux classifier\n","        if self.last_needed_block >= 2:\n","            block2 = [MaxPooling2d(pool_size=(3,3), stride=2),\n","                self.inception.layers[63],   #inception.Mixed_5b,\n","                self.inception.layers[86],   #inception.Mixed_5c,\n","                self.inception.layers[100],  #inception.Mixed_5d,\n","                self.inception.layers[132],  #inception.Mixed_6a,\n","                self.inception.layers[164],  #inception.Mixed_6b,\n","                self.inception.layers[196],  #inception.Mixed_6c,\n","                self.inception.layers[228],  #inception.Mixed_6d,\n","                self.inception.layers[248],  #inception.Mixed_6e,\n","            ]\n","            self.blocks.append(Sequential(block2))\n","\n","        # Block 3: aux classifier to final avgpool\n","        if self.last_needed_block >= 3:\n","            block3 = [\n","                self.inception.layers[276], #inception.Mixed_7a,\n","                self.inception.layers[279], #inception.Mixed_7b,\n","                self.inception.layers[307], #inception.Mixed_7c,\n","            ]\n","            self.blocks.append(Sequential(block3))\n","\n","        # Adding last average pooling layer\n","        if self.last_needed_block >= 4:\n","            block4 = [ AveragePooling2D() ]\n","            self.blocks.append(Sequential(block4))\n","\n","    def call(self, inp): # inp : torch.autograd.Variable . Input tensor of shape Bx3xHxW. Values are expected to be in range (0, 1)\n","        outp = []\n","        x = inp\n","\n","        if self.resize_input:\n","            x = UpSampling2D(size=(299, 299), mode='bilinear')(x)\n","\n","        if self.normalize_input:\n","            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n","\n","        for idx, block in enumerate(self.blocks):\n","            x = block(x)\n","            if idx in self.output_blocks:\n","                outp.append(x)\n","            if idx == self.last_needed_block:\n","                break\n","\n","        return outp"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTLuaP-odetK","executionInfo":{"status":"ok","timestamp":1626962657843,"user_tz":-120,"elapsed":40,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["# Scale an array of images to a new size\n","def scale_images(images, new_shape):\n","    images_list = list()\n","    for image in images:\n","      # resize with nearest neighbor interpolation\n","      new_image = resize(image, new_shape, 0)\n","      # store\n","      images_list.append(new_image)\n","    return np.asarray(images_list)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVeizDQEvMAc","executionInfo":{"status":"ok","timestamp":1626962657844,"user_tz":-120,"elapsed":40,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["def get_activations(files, model, batch_size=1, dims=64, verbose=False):\n","    if len(files) % batch_size != 0:\n","        print(('Warning: number of images is not a multiple of the '\n","               'batch size. Some samples are going to be ignored.'))\n","    if batch_size > len(files):\n","        print(('Warning: batch size is bigger than the data size. '\n","               'Setting batch size to data size'))\n","        batch_size = len(files)\n","\n","    n_batches = len(files) // batch_size\n","    n_used_imgs = n_batches * batch_size\n","\n","    pred_arr = np.empty((n_used_imgs, dims))\n","\n","    for i in tqdm(range(n_batches)):\n","        if verbose:\n","            print('\\rPropagating batch %d/%d' % (i + 1, n_batches), end='', flush=True)\n","        start = i * batch_size\n","        end = start + batch_size\n","\n","        img = np.array([imread(str(f)).astype(np.float32) for f in files[start:end]])\n","        \n","        # Resizing the input in order to have the same dimension of the output\n","        img = scale_images(img, (250,166,3))\n","\n","        # Reshape to (n_images, 3, height, width)\n","        img = img[:,:,:,0:3]\n","        img /= 255\n","\n","        batch = tf.cast(tf.convert_to_tensor(img), dtype=tf.float32)\n","\n","        pred = model.predict(batch)[0]\n","\n","        pred_arr = pred.transpose(0, 2, 3, 1).reshape(batch_size*pred.shape[2]*pred.shape[3],-1)\n","\n","    if verbose:\n","        print(' done')\n","\n","    return pred_arr # numpy array of dimension (num images, dims) that contains the activations of the given tensor when feeding inception with the query tensor."],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yt4w0cmkwLDj","executionInfo":{"status":"ok","timestamp":1626962657845,"user_tz":-120,"elapsed":40,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["# Numpy implementation of the Frechet Distance.\n","def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n","    \"\"\" The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n","    and X_2 ~ N(mu_2, C_2) is\n","            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n","    Stable version by Dougal J. Sutherland.\n","    Params:\n","    -- mu1   : Numpy array containing the activations of a layer of the\n","               inception net (like returned by the function 'get_predictions')\n","               for generated samples.\n","    -- mu2   : The sample mean over activations, precalculated on an\n","               representative data set.\n","    -- sigma1: The covariance matrix over activations for generated samples.\n","    -- sigma2: The covariance matrix over activations, precalculated on an\n","               representative data set.\n","    Returns:\n","    --   : The Frechet Distance.\n","    \"\"\"\n","\n","    mu1 = np.atleast_1d(mu1)\n","    mu2 = np.atleast_1d(mu2)\n","\n","    sigma1 = np.atleast_2d(sigma1)\n","    sigma2 = np.atleast_2d(sigma2)\n","\n","    assert mu1.shape == mu2.shape, \\\n","        'Training and test mean vectors have different lengths'\n","    assert sigma1.shape == sigma2.shape, \\\n","        'Training and test covariances have different dimensions'\n","\n","    diff = mu1 - mu2\n","\n","    # Product might be almost singular\n","    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n","    if not np.isfinite(covmean).all():\n","        msg = ('fid calculation produces singular product; '\n","               'adding %s to diagonal of cov estimates') % eps\n","        print(msg)\n","        offset = np.eye(sigma1.shape[0]) * eps\n","        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n","\n","    # Numerical error might give slight imaginary component\n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError('Imaginary component {}'.format(m))\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return abs(diff.dot(diff) + np.trace(sigma1) +\n","            np.trace(sigma2) - 2 * tr_covmean)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKh4iVBywY4o","executionInfo":{"status":"ok","timestamp":1626962657845,"user_tz":-120,"elapsed":40,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}}},"source":["# Calculation of the statistics used by the FID.\n","def calculate_activation_statistics(files, model, batch_size=1, dims=64, verbose=False):\n","    act = get_activations(files, model, batch_size, dims, verbose) # this function will give the output of Inception\n","    mu = np.mean(act, axis=0) # mean of the activations\n","    sigma = np.cov(act, rowvar=False) # covariance of the activations\n","    return mu, sigma\n","\n","\n","# Calculates the SIFID of two paths\n","def calculate_sifid_given_paths(path1, path2, batch_size, dims):\n","    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n","\n","    model = InceptionV3([block_idx])\n","\n","    path1 = pathlib.Path(path1)\n","    path2 = pathlib.Path(path2)\n","\n","    fid_values = []\n","    Im_ind = []\n","    for i in range(1):\n","        m1, s1 = calculate_activation_statistics([path1], model, batch_size, dims)\n","        m2, s2 = calculate_activation_statistics([path2], model, batch_size, dims)\n","        fid_values.append(calculate_frechet_distance(m1, s1, m2, s2))\n","    return fid_values"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykWqN8Srx2QA","executionInfo":{"status":"ok","timestamp":1626964570184,"user_tz":-120,"elapsed":2747,"user":{"displayName":"michela proietti","photoUrl":"","userId":"16887166090357250035"}},"outputId":"58eeb418-5cd9-450f-91bf-17e0674b5580"},"source":["# Path definition based on the student:\n","persona = \"m\"   # 'l' Luca, 'm' Michela, 's' Sofia\n","if persona==\"l\":\n","    orig = \"/content/drive/MyDrive/Sapienza Magistrale/Corsi Attuali/Vision and Perception/Progetto V&P Condiviso/Finale/\"\n","elif persona==\"m\":\n","    orig = \"/content/drive/MyDrive/VP/Project/\"\n","elif persona==\"s\":\n","    orig = \"/content/drive/MyDrive/ColabNotebooks/VISIONS & PERSPECTIVE/Finale/\"\n","else:\n","    sys.exit(\"Wrong User and it is impossible to define the directory. Try Again.\")\n","\n","path1 = orig+'Images/frattura_editing.png'\n","path2 = orig+'Testing/leg/editing/inject_at_7.png'\n","\n","sifid_values = calculate_sifid_given_paths(path1, path2, 1, 192)\n","\n","sifid_values = np.asarray(sifid_values,dtype=np.float32)\n","np.save('SIFID', sifid_values)\n","print()\n","print('SIFID: ', sifid_values.mean())"],"execution_count":20,"outputs":[{"output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n","100%|██████████| 1/1 [00:00<00:00,  3.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","SIFID:  0.00028983405\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}